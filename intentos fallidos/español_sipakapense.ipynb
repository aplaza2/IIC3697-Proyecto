{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96PapqzwAUkB"
   },
   "source": [
    "# Experimentos de Traducci√≥n Espa√±ol -> Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_FHD9kw266t",
    "outputId": "69a8c951-cbb9-443c-b074-99720455b3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yheiPHx8YN0m"
   },
   "outputs": [],
   "source": [
    "lang = \"es\"\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5rb6MT0A5mD"
   },
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yFaCwcFLx9HV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hg_qHsB95EzQ"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NF14LcemARSd"
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate peft transformers sentencepiece datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7-HRnAKmA8LR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8Nds_8Y1Mm"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smVHNrAeY5ck"
   },
   "source": [
    "Dataset se puede encontrar en el [siguiente enlace](https://github.com/transducens/mayanv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1ViYHauY4v3",
    "outputId": "4cd7ae6c-ea1c-40e1-ca7d-b7975a694164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mayanv'...\n",
      "remote: Enumerating objects: 177, done.\u001b[K\n",
      "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
      "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
      "remote: Total 177 (delta 22), reused 157 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (177/177), 1.35 MiB | 20.39 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/transducens/mayanv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ghoF4-noMjaC"
   },
   "outputs": [],
   "source": [
    "def load_lines(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def generate_dataset(language, train_folder=\"train\", test_folder=\"test\", base_path=\"mayanv/MayanV\"):\n",
    "    # Rutas\n",
    "    train_lang_path = f\"{base_path}/{language}/{train_folder}/data.{language}\"\n",
    "    test_lang_path = f\"{base_path}/{language}/{test_folder}/data.{language}\"\n",
    "\n",
    "    train_es_path = f\"{base_path}/{language}/{train_folder}/data.es\"\n",
    "    test_es_path = f\"{base_path}/{language}/{test_folder}/data.es\"\n",
    "\n",
    "    train_src = load_lines(train_lang_path)\n",
    "    train_tgt = load_lines(train_es_path)\n",
    "\n",
    "    test_src = load_lines(test_lang_path)\n",
    "    test_tgt = load_lines(test_es_path)\n",
    "\n",
    "    # Crea datasets\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_src, \"target\": train_tgt})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_src, \"target\": test_tgt})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PmYZbWQ1kPpQ"
   },
   "outputs": [],
   "source": [
    "def invertir_input_target(example):\n",
    "    return {\n",
    "        \"input\": example[\"target\"],\n",
    "        \"target\": example[\"input\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "75f1f53577dc413bb9531134f30b2a18",
      "4c4d9f11f4e9455485d4535a9de8fbe7",
      "0b63da0fca2e427e9409afdb8813d2dd",
      "bbb0aae1bbb3425794ac1e54b143a511",
      "14428ca2ae2f4cae9a61e36077b3fd5d",
      "89eb952e72cd4e5cb96437df91ae2595",
      "963c11ae52b54eac93471ca437e77fcc",
      "f3e5a39675684e49bd546069b3fcf4e1",
      "72bb7f6ee09c4900a2843296bdea38c2",
      "73859228b0c6443591313319489d3958",
      "0894c8abd72a4f55bafbf43e48b3cf16",
      "8b90706d17134b2fb2d45706b9b0b730",
      "9c0e0a87b3554777880a3b1aea8e2479",
      "fee9bcb5df0c418a9b1f8e82003628ea",
      "36f8a2623f0048c183e1c4d6285b54b0",
      "bedec5b4140345f89c918544ebc2b16e",
      "d657e58523fb46e093cd61c19523d246",
      "d20158e1d2ad456f9342b866ada2a128",
      "932aed3a3e8e4ee8be780e796e0ee4a2",
      "818bfba706f84308a0ab92e53dc6d573",
      "676592b12d4f472bbe6bdc4ee2ecc343",
      "967b981bc7be4d53a59cac4859e4f12e"
     ]
    },
    "id": "4l0xcZcOgsmx",
    "outputId": "1d073710-e100-4a2c-9759-e18f7611ac72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f1f53577dc413bb9531134f30b2a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b90706d17134b2fb2d45706b9b0b730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_qum_dataset = generate_dataset(\"qum\", train_folder=\"test\", test_folder=\"dev\")\n",
    "es_qum_dataset = DatasetDict({\n",
    "    \"train\": es_qum_dataset[\"train\"].map(invertir_input_target),\n",
    "    \"test\": es_qum_dataset[\"test\"].map(invertir_input_target),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6AMhN2GZpNt"
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4fo2s-t9Zozn"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 128\n",
    "\n",
    "    inputs = finetune_tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    targets = finetune_tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def translate_text(model, tokenizer, text, src_lang, tgt_lang):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    forced_bos_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "    # Detecta el dispositivo (GPU o CPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokeniza y mueve a la misma device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Generaci√≥n\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def build_and_save_split_en_only(split, model, tokenizer, output_dir):\n",
    "    input_path = os.path.join(output_dir, f\"{split}.{lang}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"w\", encoding=\"utf-8\") as f_en:\n",
    "        print(f\"üîÑ Traduciendo y guardando split '{split}' al {lang}...\\n\")\n",
    "\n",
    "        for example in tqdm(es_qum_dataset[split]):\n",
    "            es_text = example[\"input\"].replace(\"#qum#\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                en_text = translate_text(model, tokenizer, es_text, \"spa_Latn\", f\"{lang}_Latn\")\n",
    "                en_text = \"#qum# \" + en_text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error traduciendo: {es_text}\\n{e}\")\n",
    "                en_text = \"#qum#\"\n",
    "\n",
    "            f_en.write(en_text + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Archivo '{split}.{lang}' guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6y8Xh2RBAmS"
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWCp3bkc27u"
   },
   "source": [
    "### 1. Modelo con Finetuning Espa√±ol-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "ca9b10d76c3e445592765cec138b4ce4",
      "876d7e38e2d1483dbeaa2523a10e617c",
      "93bb7ba60c164ae6a2cfddbfae7590bf",
      "a87684b0dbe447dfa76c8b880582e359",
      "aa63414494ed4f52a628d5e2ef73462a",
      "9d55817aaf1d401f8cd8b61be896ccf3",
      "ad778e9033ad4d7f90dbe2af7b0da7ed",
      "0cac41922f484a38905b8db3d6ecfd8c",
      "d846e16db34349a898d1022f6cd463ae",
      "e82fad3f38f04ffea1a317196b8e61e9",
      "ddd284bff4c04681b995db6477368742",
      "e536ba063ba445b190e7ef9a2c247450",
      "a30d34414345472498f9131db8c1ad8e",
      "a2e7bc7cb6384a429eabac843f74cb09",
      "d0d314714ab54c6a99e28fd321cdcbb6",
      "8183292b381542619ddabceadf4c6936",
      "f864cd826b134ec9b79759b30ec4251b",
      "bf046dd01c42450cae3b8c5eafeba7a4",
      "06163cdc26c84405887a843c8a2b3375",
      "811cb9b40f5a45a4ac3d07b0e379b381",
      "0b6bae1b24634fc0aa187c8f94f47171",
      "7b6f321f164a40e68d2d2106f889f2bc",
      "acc205456017466590d3c8141714f119",
      "2e92d205f3834703880dc6ad585b14e2",
      "db6d57d1df374d219849f58d26173266",
      "ecd43ffc3d1d47648f4e0bde842f749c",
      "61a0cdbc69124eeb82407faa264d65ff",
      "6d84305a24ad41cdb0d9bed1cb32f80c",
      "7b0ad179181e4da78d6f25b244911c03",
      "f3a2426385174326b3c4e3ea0d8af54f",
      "5c3b278375f945a1915be1bdce2a0b58",
      "c34c45375c8945ca9d1e6dd94de0d843",
      "30f148e1e6d14831aa132a0c9fcf7d03",
      "6ab7be4a809242b8990254a8d21cc336",
      "e366e3b8b5de478ba1fa38e6368c2959",
      "b7551366161a47588b6928a8fdc9ce3a",
      "99edde7f5fa04fcb9b330882effe4221",
      "c2850b1c08c64495af1f54848cf6658b",
      "91e266b12e5141438dccaf2183a3fed1",
      "f1419a6dbdfa495484dee6419f6adcf7",
      "0a217e84709f451c9a7fdf7a9e71a8fd",
      "444c1c5ce96143a998ad9331893707ed",
      "f3c86cc3635e48b695e5d22b648e22ed",
      "3550c1dfcffb45a6bd47c45543790b2f",
      "bff8cb10f34b4621b4619b0766b6a080",
      "339ab6fddeeb41b8b77631603bf0f8dd",
      "b41330f4ceef420b8204cfbee501f3f9",
      "02eed0149455486c96337ea509a5433f",
      "6a859086b0154bd5a671aac09a405e89",
      "13bbd56d358e4dd3a23e188177c7f60f",
      "1e959cdc34fc40ff89e541fe04a791ca",
      "6a88f66744014f0cb98b6b87a434984c",
      "0a1b864155144b2a95e9b335e2ff3948",
      "df225fc19c7745388f8c192b525fe17e",
      "a0d4f559250e4aa38a5b4407e9f0f5bb",
      "0ff3f751aa0b41208253ce530376043b",
      "a81a77d6eb56419a8a9a8f98e2679c94",
      "2810873dbaca4a938984453ac5d60d9f",
      "c66c862a45ac4d6e86e09dcf2d9db661",
      "00395a2c296e4e369310b369315aae5c",
      "daf760c73269404a804d24dbb0433234",
      "03a5d0e4010344e9b07419fbdb2e66e1",
      "2d4fdf464033412eafdd0f3a734c4b92",
      "44d212e8c649412f88511c238b54971c",
      "9ce0c2dd7ba044db8a0436ec500d4a66",
      "27797bb36cf84c7f96d9614ba4c7f911",
      "0b5b2de3a3654ead95fbd3cc9445fdbf",
      "c240f3eba4154c008bb59daa00d6685c",
      "e5a17d4c16ca4d8c927621da50d80c9b",
      "c78dde28431048e2b0fab6136bd90d83",
      "adfce887847c49ec9bd6085e4306e4da",
      "a0d709932f8b4f059e78e3245593f6c9",
      "4543a7a83e824d76afec08f852093eb2",
      "e5240f8aa401456f9cdac6dc8322d13b",
      "170feff6a38d4d75810b1d43bcd1b2e1",
      "87d005ea296949108f249e9e78498c15",
      "f55c85198a5c4631ab8f9a7a4a68b5c7",
      "a498333b98cd496f94024b5d0badf964",
      "d308b04c0e52442dbd1b8ee3c2c70e32",
      "0b044385761c4b689851428412c575e9",
      "b7612e6b786348c8bd5caf1d2df7d1af",
      "4cd30a61922f4edca51db7aee5d0c1af",
      "ec127206b7244605be4263b5e81adaca",
      "a13fa76312ed40f7812dca3cc9ee6633",
      "8add6137a0ba43edbe7a382261f06fd0",
      "cf2c4864b80f4f4e975ba99e9e858e1c",
      "c80401baa7c74313ae8e6ae65dfe1561",
      "2101f21589774493905e217786601da0"
     ]
    },
    "id": "j0zKtLz_K7Uc",
    "outputId": "760e6ada-17f0-4ca6-b049-b25ca0f069ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9b10d76c3e445592765cec138b4ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e536ba063ba445b190e7ef9a2c247450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc205456017466590d3c8141714f119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab7be4a809242b8990254a8d21cc336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff8cb10f34b4621b4619b0766b6a080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff3f751aa0b41208253ce530376043b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5b2de3a3654ead95fbd3cc9445fdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a498333b98cd496f94024b5d0badf964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configurar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Aplicar QLoRA\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TAeIyJ4pOZM",
    "outputId": "7a5506c5-1c46-492a-a419-a39e31401ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A√±adir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "finetune_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "finetune_model.resize_token_embeddings(len(finetune_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "562637d4dd854b77b73455ac5723ef82",
      "01db8222639442c5960dd47f29cc7629",
      "83ddbf0e1c8d43a681e8afc9b2c81863",
      "85e442f65b74413eb8b5239384152529",
      "e2cb9a162d3a4eb2a2517e3f9f0706ea",
      "ad7134aedc164ca68090dfd362a3f300",
      "cdf0649e21f14cdc85cad074a8b28ffe",
      "3b41065002a34ba8b1e30965c15e0bab",
      "66704ff098a84f33b3dd636b5e999662",
      "20b2a45360364d8a89ce2f8fac0aa4bb",
      "89533ffdfebe4bb2bad8808803489a45",
      "92304d00cb46419db27e698fe2e4b2a7",
      "a22db235acd34602bf5e8e18d707ca1a",
      "eb13655959d94f5598a68ca7fe4da48a",
      "65f2600e203f435383832bb545a20a8b",
      "2b9fb9da217244a0817731bec4bd3158",
      "8b30d766339e41be8f9a8dc4a6de243a",
      "76a58b04dd87463a96168b9d50ef308c",
      "9c82cf41b63a4a48b5cddbd13e078c72",
      "eba96b2d3a954a2e8d77de5cd75fb0d7",
      "4239b1a89bfb4bca8af816c5b14bae8b",
      "fc79f235e2654862853ca14b832fd694"
     ]
    },
    "id": "R2iPhpoQnSc2",
    "outputId": "adb6a5e6-5dc5-4857-c4e7-dd6c59eee5da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562637d4dd854b77b73455ac5723ef82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92304d00cb46419db27e698fe2e4b2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = es_qum_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = es_qum_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aXgDM4lAnagN",
    "outputId": "5599e58a-98ad-4a0f-92a3-42870bc49e98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-14-1857230931.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 30:42, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>7.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>7.029900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qlora_finetuned_qum_nllb/tokenizer_config.json',\n",
       " './qlora_finetuned_qum_nllb/special_tokens_map.json',\n",
       " './qlora_finetuned_qum_nllb/sentencepiece.bpe.model',\n",
       " './qlora_finetuned_qum_nllb/added_tokens.json',\n",
       " './qlora_finetuned_qum_nllb/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_finetuned_qum_nllb\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_qlora\",\n",
    "    save_total_limit=2,\n",
    "    bf16=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=finetune_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "finetune_model.save_pretrained(\"./qlora_finetuned_qum_nllb\")\n",
    "finetune_tokenizer.save_pretrained(\"./qlora_finetuned_qum_nllb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc2QH1HJcuKn"
   },
   "source": [
    "## Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9jCIC05CCG_"
   },
   "source": [
    "### M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "57e5f040707e47fbb618d502bb479342",
      "3a218e837fde40b8aad89e05009e1f8f",
      "62d38de41b4e439292bff470838e2b88",
      "24f44390a11444a5a296d244a6e22094",
      "8b8bbec00f47442e9ab06f61cba7e5ce",
      "943ad60db64b46078d5b50d57563b355",
      "c378226b97a049419b5c09c26d780295",
      "9cad96a7aa594b5396df6f060631b423",
      "dc4320638a874411b7ccc5e0ba460e88",
      "2c4b86f8fab947f1a006ff372a13aed1",
      "58398858fd7f462a96485b0c89d0c312",
      "7f9d8d4b764242bf8fe79151a66b8aab",
      "0be3bf374bdd4cee942c27791ef1d4b0",
      "d2be8429c47e4c57ad3d4c8f1c2d6803",
      "f099f76ce6404615991f93ffbd5b47f6",
      "57a07fae7b984b559861a59a1e00068a",
      "1d31a5c5a1394d26aa532e1c2b72350e",
      "12ddcfa6facf470ba2060e6caeb10499",
      "9549b750305a4c1e8550b35605bbfd91",
      "62c2cb75e2904252863073f35fa26791",
      "2d43acc32e1b46e887176ea985bd26ad",
      "cbcc76581e3e46ee8601e248bde6901c",
      "0df84b4d54d74cb392767b1d381f03e6",
      "97fb57c602844bd5a526eaa5ec7bed41",
      "aa87cf89515143dfac59e7161500f53c",
      "154859fd77b44a4088945a995246692e",
      "1141fb4ae247425986aeb7e2774b1372",
      "152d7b074e2a4fcb999717b30cf6906a",
      "c75b76b393f04ad394c8b837c26af77e",
      "5674b06f7ed448bdbb2b499c7581cd4e",
      "a34f962f63f74918a88b6c8d216e4fe4",
      "2dc746b6bf0d464894a4a117bb71be26",
      "d277a7ed509d4755a7603c28d8714c62",
      "87c6ca636e76430f8472f1ee5219cdfd",
      "e9c18f77be10445abdb86aef35dae890",
      "ef9bec837ff14896a0c8112fc14ef19f",
      "1b70dd3ba6814558afbf209073a939a7",
      "2486885ecd5641e68d3d78581b98139f",
      "dc1342fd00374b4cbbd80f98be14c942",
      "e85fc722245540db9823a08f46a4da6b",
      "71260b5d2b044fa7a3eaf46410b5aa65",
      "cb56e543e57a41e4b976fac7ee8e50b2",
      "84329faf337647fd8cf715a5b4c4193b",
      "a4455aec89ee456fa7a84051296f1e54",
      "833f044047554b499d219f1731086fd9",
      "0e882ca6a39f477789e50f8138b83c30",
      "2f3d62bd87a74b06bb4d66002cd1e428",
      "fa8c2bfcacd74439b6c8911a20eae6c6",
      "34af87e711aa4f86bd5ddcb1b1612e45",
      "8bfed0c37fcb4ea3a4b0e5b311e9ce77",
      "bdefe7f6d5ee4340ba6bbf958317f2fa",
      "66b3313fa9174ed6963d96a348adba92",
      "70eb8d15ea7447fa8d0e4430241a77d8",
      "d2c19344abb04599a89f61ab57ddd39d",
      "6a00d4f3208a48bcb468da80ff8750e5"
     ]
    },
    "id": "YEoaaYAZCDnX",
    "outputId": "20d27755-6762-4f73-a738-a672c98a6d83"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e5f040707e47fbb618d502bb479342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9d8d4b764242bf8fe79151a66b8aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df84b4d54d74cb392767b1d381f03e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c6ca636e76430f8472f1ee5219cdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833f044047554b499d219f1731086fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coKh9lECcw45"
   },
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjB3SBkqn1JG"
   },
   "source": [
    "#### 1. Con Finetuning Espa√±ol-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uBCVbR3obZG",
    "outputId": "0d0098d7-871b-4086-e00b-95b7b0b94596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Traduciendo: Espa√±ol ‚Üí Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321/321 [04:32<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_intermediate = []\n",
    "references_intermediate = []\n",
    "\n",
    "print(f\"\\nüîπ Traduciendo: Espa√±ol ‚Üí Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(es_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, input_text, \"spa_Latn\", \"qum\")\n",
    "\n",
    "    predictions_intermediate.append(pred)\n",
    "    references_intermediate.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qVv0v6pUvm-e"
   },
   "outputs": [],
   "source": [
    "clean_pred_intermediate = [pred.removeprefix(\"qum\").strip() for pred in predictions_intermediate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2roPLoJoy2L",
    "outputId": "810d892a-7dd0-4dd0-8a77-203e630e476c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# La esposa de don Jesus solo utiliza buena le√±a\n",
      "> Referencia:  wu rxqiil ta º xuux xaq utz laj si º kchuknik chre\n",
      "> Predicci√≥n:  ri ajk ºal ri aliit\n",
      "\n",
      "> Entrada:     #qum# Todo la arveja se\n",
      "> Referencia:  Njeel ri xkarawa ºn xpoch ºik rech chenim xchaq ºjik\n",
      "> Predicci√≥n:  kirb ºal ri rb ºanik ri ajk ºalb ºal\n",
      "\n",
      "> Entrada:     #qum# En nuestro pueblo ya no se usa nuestro idioma\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolb ºaal\n",
      "> Predicci√≥n:  ri rka ºyxik ri rka ºyxik\n",
      "\n",
      "> Entrada:     #qum# El estudiante est√° aprendiendo como\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri su º\n",
      "> Predicci√≥n:  ri ch ºiij xb ºanik\n",
      "\n",
      "> Entrada:     #qum# La mujer estaba abrazada por su marido\n",
      "> Referencia:  Ri ixoq tz ºulmaj rum ri rchjiil\n",
      "> Predicci√≥n:  xb ºeel ri rq ºo ºj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {es_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {es_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicci√≥n:  {clean_pred_intermediate[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcM-8kilozFz",
    "outputId": "d9505ad1-0ca4-4de9-9172-f88f66e8eb2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√©tricas BLEU ===\n",
      "BLEU-1: 16.15\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== M√©tricas ROUGE ===\n",
      "ROUGE-1:   15.21\n",
      "ROUGE-2:   0.40\n",
      "ROUGE-L:   14.76\n",
      "ROUGE-Lsum:14.76\n",
      "\n",
      "=== Otras M√©tricas ===\n",
      "METEOR:     8.04\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_intermediate, references=[[ref] for ref in references_intermediate])\n",
    "\n",
    "print(\"\\n=== M√©tricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== M√©tricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Otras M√©tricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MTsr6mJBWrm",
    "outputId": "eb2b42f9-1f1b-4c03-94c5-7c8bd8d50eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\n"
     ]
    }
   ],
   "source": [
    "with open(f\"results_es_qum_finetune_intermediate.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_intermediate, references_intermediate):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(\"‚úÖ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g5rb6MT0A5mD",
    "R6AMhN2GZpNt",
    "Q5ukIdzqYhCo",
    "o6ngW-l5c1aq",
    "qjWCp3bkc27u",
    "dazLUZlhKKHk",
    "tc2QH1HJcuKn",
    "U9jCIC05CCG_",
    "coKh9lECcw45",
    "ljNvMeB3ezyA",
    "rjB3SBkqn1JG",
    "y_UFY-dToFsb",
    "fCaQTajaoPyr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
