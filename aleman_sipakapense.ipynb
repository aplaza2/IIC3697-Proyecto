{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96PapqzwAUkB"
   },
   "source": [
    "# Experimentos de Traducción Alemán -> Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_FHD9kw266t",
    "outputId": "6b73b20c-6d93-43dc-e87f-7a3876fbd965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yheiPHx8YN0m"
   },
   "outputs": [],
   "source": [
    "lang = \"de\"\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5rb6MT0A5mD"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yFaCwcFLx9HV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hg_qHsB95EzQ"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NF14LcemARSd"
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate peft transformers sentencepiece datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7-HRnAKmA8LR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8Nds_8Y1Mm"
   },
   "source": [
    "## Datasets\n",
    "!REQUIERE SUBIR LOS ARCHIVOS DEL DATASET SINTETICO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smVHNrAeY5ck"
   },
   "source": [
    "Dataset se puede encontrar en el [siguiente enlace](https://github.com/transducens/mayanv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1ViYHauY4v3",
    "outputId": "a146af8e-a028-4b0a-8f17-b258c0c26e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mayanv'...\n",
      "remote: Enumerating objects: 177, done.\u001b[K\n",
      "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
      "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
      "remote: Total 177 (delta 22), reused 157 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (177/177), 1.35 MiB | 23.11 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/transducens/mayanv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ghoF4-noMjaC"
   },
   "outputs": [],
   "source": [
    "def load_lines(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def generate_dataset(language, train_folder=\"train\", test_folder=\"test\", base_path=\"mayanv/MayanV\"):\n",
    "    # Rutas\n",
    "    train_lang_path = f\"{base_path}/{language}/{train_folder}/data.{language}\"\n",
    "    test_lang_path = f\"{base_path}/{language}/{test_folder}/data.{language}\"\n",
    "\n",
    "    train_es_path = f\"{base_path}/{language}/{train_folder}/data.es\"\n",
    "    test_es_path = f\"{base_path}/{language}/{test_folder}/data.es\"\n",
    "\n",
    "    train_src = load_lines(train_lang_path)\n",
    "    train_tgt = load_lines(train_es_path)\n",
    "\n",
    "    test_src = load_lines(test_lang_path)\n",
    "    test_tgt = load_lines(test_es_path)\n",
    "\n",
    "    # Crea datasets\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_src, \"target\": train_tgt})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_src, \"target\": test_tgt})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PmYZbWQ1kPpQ"
   },
   "outputs": [],
   "source": [
    "def invertir_input_target(example):\n",
    "    return {\n",
    "        \"input\": example[\"target\"],\n",
    "        \"target\": example[\"input\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "287c42ad482a4a70a425a0429b297da3",
      "f332d5363dac45268e56f00f33a727ba",
      "e5335619d8b1454bb385b077f163cd7b",
      "e4a8e77513fb4871a1003443f3a9f8d8",
      "1628e047674844c99fc8bcb050a84039",
      "09662fe917af4f6293a25021f62d45f1",
      "e83a3ec99e5a48a784e28168c378b0c6",
      "4e2007ddea334c2e910c602264ba39d3",
      "f3371d1ff3fb45bea6c88df54514db7b",
      "889ffb5698344cb1964c156ec78bf68b",
      "7b359563e816484cb2b0174218ae101e",
      "4a2b661f0adf4659800797b98a539442",
      "50950572081f4348903a231d3f54d91f",
      "7f29e5f9c390472fb6cdcfdf2753897c",
      "dc7c13393b88473d800867e024e134ed",
      "7fe8e662d29049a195e98381fd835fad",
      "0ee83cd0f2674739b39e761124b98218",
      "f25bccf8989843859d378084023eeea5",
      "f90c61f8e05e41eeb9af5664b4ef19c5",
      "34f3d4d94fce4c448557ca257343dcc5",
      "65d724ae02794dd4bfc003374aacd681",
      "a3953895b75346e09910f66dc1746f3f"
     ]
    },
    "id": "4l0xcZcOgsmx",
    "outputId": "1a258367-5f98-46a9-fee7-31cf34ea0c86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287c42ad482a4a70a425a0429b297da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2b661f0adf4659800797b98a539442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_qum_dataset = generate_dataset(\"qum\", train_folder=\"test\", test_folder=\"dev\")\n",
    "es_qum_dataset = DatasetDict({\n",
    "    \"train\": es_qum_dataset[\"train\"].map(invertir_input_target),\n",
    "    \"test\": es_qum_dataset[\"test\"].map(invertir_input_target),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e-p1Rq94bTJL"
   },
   "outputs": [],
   "source": [
    "def generate_en_qum_dataset_from_files(\n",
    "    data_dir=\"./\",\n",
    "    base_path=\"mayanv/MayanV\",\n",
    "    language=\"qum\",\n",
    "    train_folder=\"test\",\n",
    "    test_folder=\"dev\",\n",
    "    lang=lang\n",
    "):\n",
    "    # Paths a los archivos .en\n",
    "    train_en_path = os.path.join(data_dir, f\"train.{lang}\")\n",
    "    test_en_path = os.path.join(data_dir, f\"test.{lang}\")\n",
    "\n",
    "    # Paths a los archivos .qum\n",
    "    train_qum_path = os.path.join(base_path, language, train_folder, \"data.qum\")\n",
    "    test_qum_path = os.path.join(base_path, language, test_folder, \"data.qum\")\n",
    "\n",
    "    # Carga las líneas\n",
    "    train_en = load_lines(train_en_path)\n",
    "    test_en = load_lines(test_en_path)\n",
    "\n",
    "    train_qum = load_lines(train_qum_path)\n",
    "    test_qum = load_lines(test_qum_path)\n",
    "\n",
    "    # Validación\n",
    "    assert len(train_en) == len(train_qum), f\"Train mismatch: {len(train_en)} vs {len(train_qum)}\"\n",
    "    assert len(test_en) == len(test_qum), f\"Test mismatch: {len(test_en)} vs {len(test_qum)}\"\n",
    "\n",
    "    # Combina en datasets Hugging Face\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_en, \"target\": train_qum})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_en, \"target\": test_qum})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })\n",
    "\n",
    "\n",
    "en_qum_dataset = generate_en_qum_dataset_from_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6AMhN2GZpNt"
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4fo2s-t9Zozn"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 128\n",
    "\n",
    "    inputs = finetune_tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    targets = finetune_tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def translate_text(model, tokenizer, text, src_lang, tgt_lang):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    forced_bos_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "    # Detecta el dispositivo (GPU o CPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokeniza y mueve a la misma device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Generación\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def build_and_save_split_en_only(split, model, tokenizer, output_dir):\n",
    "    input_path = os.path.join(output_dir, f\"{split}.{lang}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"w\", encoding=\"utf-8\") as f_en:\n",
    "        print(f\"🔄 Traduciendo y guardando split '{split}' al {lang}...\\n\")\n",
    "\n",
    "        for example in tqdm(es_qum_dataset[split]):\n",
    "            es_text = example[\"input\"].replace(\"#qum#\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                en_text = translate_text(model, tokenizer, es_text, \"spa_Latn\", f\"{lang}_Latn\")\n",
    "                en_text = \"#qum# \" + en_text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error traduciendo: {es_text}\\n{e}\")\n",
    "                en_text = \"#qum#\"\n",
    "\n",
    "            f_en.write(en_text + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Archivo '{split}.{lang}' guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5ukIdzqYhCo"
   },
   "source": [
    "## Crear dataset sintético\n",
    "!SOLO EJECUTAR SI NO SE TIENE EL DATASET TODAVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOjTDQjmYq8F"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "mid_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "mid_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "mid_tokenizer.src_lang = \"spa_Latn\"\n",
    "mid_forced_bos_id = mid_tokenizer.convert_tokens_to_ids(f\"{lang}_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U4gFjfFYots"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/en_qum_dataset\"\n",
    "\n",
    "build_and_save_split_en_only(\"test\", mid_model, mid_tokenizer, output_dir)\n",
    "build_and_save_split_en_only(\"train\", mid_model, mid_tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6y8Xh2RBAmS"
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWCp3bkc27u"
   },
   "source": [
    "### 1. Modelo con Finetuning Español-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "45d7b9b8a4f44fb28b4815a517e13b46",
      "c35422a7ee3d49cda8ceb864581d4a81",
      "0b78e318f34844aa8fba5845c4efae2e",
      "721976d22f3c4d2f816cd197a66578e0",
      "5c6287bcca70490f9eea2fa879ac4f23",
      "ba02886a2ef44b8dbcb8392995d150de",
      "db676dd55d5e4b8f98fc6ea053f2a2ac",
      "467653542c3445cfbfe69e450a19cc23",
      "4caebc904f5a4361a7469c455af17884",
      "500b85daae7a4c4aa347619530133d2f",
      "ad308d47364246e39913e1746627bf4c",
      "b9ef6bec4eda4bfc997f9914f494c2dc",
      "dcf5ebb5a4714e6395d6759f6c585e10",
      "f4a21dd6e9254b21b9836c33b99bb6fe",
      "1aaa7f0de102400aabc114f7a91b896a",
      "f43537394af047138f472a15e0ba0a41",
      "8bf8b14633e44e539f9b99abdb4c16eb",
      "4a9509da6d40410ab783343cf94f2378",
      "0cabdd101a664f1aa9d3a9a1cf08f41a",
      "5961f3fd11bf437fb6b684b039e60027",
      "d808a78b28284710a288b6e37fe40042",
      "56d465f813cd4980bc1aa4fb9651a93a",
      "6b950d5596194179a60dcc637c1463d3",
      "c9b8fa8e084e4a4ea9e9885ab7c9b908",
      "0b23ba299def40b89b56ae426f2be330",
      "88d9a4209c204aab92cb284574b33182",
      "992e6d0dfa6f409d8fae07081233d0ad",
      "e2ac3c49f002475aa0d7a922e476a295",
      "f46e3e90341d4d8986454baad2a5e203",
      "e21321f2c34b429fa557aa26d91e1b46",
      "bffc8ccf42c24dd292d51a944ffd063a",
      "673e6d9767f04d0e949bea1eb065bb76",
      "8e82e64d6fc94a6d9acf7435c821282d",
      "cf8a663c50424f96ac1976f3eb0f4057",
      "c12bf570104b4fc1a9c7f81bc054dc1f",
      "d9000361b0e4402ea906bc161232f70b",
      "97936526f6c3411f8ffaf16798c3bccc",
      "e26d7543eea64dbb82addb80ffdcbc2a",
      "e6263c01bc434c3085f59f8426d05b36",
      "b23791381cbc4623b672b18ec49cea59",
      "93894d9703754b799466187d83791f37",
      "e6e710a99f9c4aafa703f1898b340fb0",
      "5a3fd3f3f2a34045ab4248a0aa643071",
      "a0334f8ab7834d2c9abfa6895b14f29a",
      "c6643f338af04382a07b034c8fba200c",
      "83c485e7823c4b6b95831a76db427a61",
      "7eb0f254256447a0a10446da602f39ec",
      "f7ccbf4f63434bf7a04dafdc54493312",
      "f604c63922d54f53b2a85d06f8a10e91",
      "50eb3864bccd4ee38a01f86f5f138d2a",
      "8bcac1e011f44a6191e58ac65aa43184",
      "e6662e624686425ab4e7fd4b692b6968",
      "b0ce17b3973c4cdab11563e7dfe11e37",
      "6fbeb3f7cd09476b914b1bc5e3ba80af",
      "4c4447976d3f4e6eb0d8d6ab2f249cf9",
      "265da2b0330d48c8b291d80c5d1e00e4",
      "4e18f036329743f29b5f0e1a19fb6a78",
      "68065c26be73458aa4c75103d1f1539d",
      "797480712497473d94644c14bc2b37c5",
      "87762598cd9546a3add4b8a061808f7a",
      "87009ba227274ae0941aa9a839f308e9",
      "4ba0dabbd14540b295d5934ab0ca0262",
      "20f684cb9bf54938b6c6af501375a798",
      "0c51e9056e3a486ba7e5150ff0f54ce9",
      "b5bba80f3ca14a5e9f7cf14047bed80b",
      "3e20d1c5a94c4ad489d7ab0774728132",
      "332c9a41cdd4400ab48a7c35124d8658",
      "8e9fe550c1694cb9b0e73d757c2272d1",
      "20f8213fde9547dca3ab322605b3d2cf",
      "23388f054b2a485996eb27a1f7b20b4f",
      "cc71045baccb4ec2bcbfb178b3f165a8",
      "db7847e493d4497b85de3c4798f7e958",
      "11a035b931f0455a8b23d99eaadf1324",
      "a6879e63af1f4e0abd03943082641b42",
      "f8ccc339167d41c7baea95fbfe4770e2",
      "9430e2abcb124c2fbf0e6a333d64588a",
      "feb38fe2eafb48a29a37f9bd9119d173",
      "e2a6c282ee88433b9d3ea49ca8d85b56",
      "a4ee6141fd284d39a9541f94905671b7",
      "b3dd9d0048634223abe543f9995acb29",
      "ddfd675aa7684c0bae3f9d310855aedf",
      "be212cb58de244b7af4ced554754aa1c",
      "2e3f4cf0869741bbb6fa62851e1b5911",
      "a8557582c1d5453cb62fb8b8f7d8af8b",
      "d3b825f2cb3246c48673b90bccfa173e",
      "e065d940d1ee4f239513e274f8bbd0d3",
      "4c78e5a8a1ad470eb3d9cec9a05f5b0a",
      "247c9847e48d4fe4a61579d3db55b066"
     ]
    },
    "id": "j0zKtLz_K7Uc",
    "outputId": "e674cd42-d930-43f9-f938-e0da28a9388f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d7b9b8a4f44fb28b4815a517e13b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ef6bec4eda4bfc997f9914f494c2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b950d5596194179a60dcc637c1463d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8a663c50424f96ac1976f3eb0f4057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6643f338af04382a07b034c8fba200c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265da2b0330d48c8b291d80c5d1e00e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332c9a41cdd4400ab48a7c35124d8658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a6c282ee88433b9d3ea49ca8d85b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configurar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Aplicar QLoRA\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TAeIyJ4pOZM",
    "outputId": "851ae719-f28d-4737-e41c-847bdbb42c4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Añadir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "finetune_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "finetune_model.resize_token_embeddings(len(finetune_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "646acfc89eef420986c58a835045cfff",
      "2fa510856af84515857eac50e70349e9",
      "01fff7aef1224190acc281cb83eb37df",
      "c545b90965724eb8ae3f33125d4cdd8e",
      "bd33100e56e14d44a31bdd69de601dec",
      "193c3794db164051b0eb915f43978e60",
      "e1905a8879c748799e9e4108669e23ff",
      "a91c4cf59c304374adb72db5006e0782",
      "71d48694f4aa4986b020545d64f2e4f6",
      "2ec6d8a6ceae4aea81e81a3f1f5eed69",
      "8c5936b0658442e28835e899eff05358",
      "b8556b14185b407c983dbfb7c70d0aac",
      "f670338d81af49bb9b716b29139d6842",
      "de836054d1cc4dd2911b67b6e90ec7d5",
      "a3b2e36aac12418ca6e92eff4b74045e",
      "2d76391ccc7345ba8c862bc03c58b8ac",
      "02677ec481534ac194bacff94044ca5d",
      "6c0b298e2d1c472699552e072736c7be",
      "f8c0c3ab47494f32b97f41e4cdf839e9",
      "635f135a4d524f95bb2e55f48297db48",
      "7824fff721f8457697092516f573adae",
      "5f3d01bd558644d2ab16560052ceacf6"
     ]
    },
    "id": "R2iPhpoQnSc2",
    "outputId": "90386d16-4cec-42b0-aa93-63849b58ec04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646acfc89eef420986c58a835045cfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8556b14185b407c983dbfb7c70d0aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = es_qum_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = es_qum_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aXgDM4lAnagN",
    "outputId": "d63c6100-db4a-4082-dffb-6d19d8d83e6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-15-1857230931.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 22:23, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>7.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>7.029600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qlora_finetuned_qum_nllb/tokenizer_config.json',\n",
       " './qlora_finetuned_qum_nllb/special_tokens_map.json',\n",
       " './qlora_finetuned_qum_nllb/sentencepiece.bpe.model',\n",
       " './qlora_finetuned_qum_nllb/added_tokens.json',\n",
       " './qlora_finetuned_qum_nllb/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_finetuned_qum_nllb\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_qlora\",\n",
    "    save_total_limit=2,\n",
    "    bf16=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=finetune_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "finetune_model.save_pretrained(\"./qlora_finetuned_qum_nllb\")\n",
    "finetune_tokenizer.save_pretrained(\"./qlora_finetuned_qum_nllb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dazLUZlhKKHk"
   },
   "source": [
    "### 2. Modelo con Finetuning Alemán-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEUkIjpj33ln",
    "outputId": "06beadb2-3598-427e-fcdc-b26a80dbd41c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "en_qum_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "en_qum_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Stma1J90dTmA",
    "outputId": "0ec76107-a1ee-455a-e774-e1307f7ff6d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Añadir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "en_qum_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "en_qum_model.resize_token_embeddings(len(en_qum_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "defaad35c34b44529ad633b94c648386",
      "98bcc29fd499436a9337afc8129a702c",
      "a13ad405acb7418d8f8d27b826ea7fe9",
      "428329930bc34f06b3b80c838ea10160",
      "92809da1ec2d4b4bb657583608ddc20e",
      "945063cfd1c34da8bae28b39e5a19661",
      "0ebabbf85d67462680e1d9d2ae2114b1",
      "879e5295284447e7b83b1af2a18739f5",
      "8a476a7d18e5476fb39e281fc4b0b206",
      "b375fa15fc654fb49972e7daafb114a6",
      "3a75c6f24a0d4d9a907969603d91a367",
      "2b19cdfca8954c6da05296a2bff6eefe",
      "7e12388c76fc42058fedf35a80d65309",
      "b7bf74531c974837a51849e0100fd121",
      "06960b7ba05c4a71adc2fcbd8ba2b932",
      "e3c8e074684e491d9f1dcbca7832914d",
      "8217bb1a8b254592b697caa227e84a0e",
      "f70a673c58934e4db442d711e8beeca8",
      "213f91aecc444a41a8230ea2572c2b18",
      "42696f5ab35d4633bd36d3a91a117cab",
      "fc1b254b68d34501b4cb45f4c18b291a",
      "941c16a5a30a429ab43958fd9b8118bc"
     ]
    },
    "id": "w1sEWghP4B7C",
    "outputId": "1adce355-7979-4cdf-d410-608779016508"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defaad35c34b44529ad633b94c648386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b19cdfca8954c6da05296a2bff6eefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function_en_qum(examples):\n",
    "    max_length = 128\n",
    "    inputs = en_qum_tokenizer(examples[\"input\"], max_length=max_length, truncation=True, padding=True)\n",
    "    targets = en_qum_tokenizer(examples[\"target\"], max_length=max_length, truncation=True, padding=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_en_qum_train = en_qum_dataset[\"train\"].map(preprocess_function_en_qum, batched=True)\n",
    "tokenized_en_qum_test = en_qum_dataset[\"test\"].map(preprocess_function_en_qum, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxivqDvt4GEf",
    "outputId": "c93a2f1a-89ef-4436-fd49-1c8f87cc766e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args_en_qum = TrainingArguments(\n",
    "    output_dir=\"./finetuned_en_qum_nllb\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=n_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_en_qum\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WWYUxQWm4IU_",
    "outputId": "21606d59-0c78-4a75-9b62-b028cbafb421"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-13-1310942191.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_en_qum = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 11:59, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>8.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.615200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>6.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>6.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>6.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>6.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>6.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>6.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>6.197900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=6.857385416666666, metrics={'train_runtime': 720.0946, 'train_samples_per_second': 20.831, 'train_steps_per_second': 2.604, 'total_flos': 796271616000000.0, 'train_loss': 6.857385416666666, 'epoch': 15.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_en_qum = Trainer(\n",
    "    model=en_qum_model,\n",
    "    args=training_args_en_qum,\n",
    "    train_dataset=tokenized_en_qum_train,\n",
    "    eval_dataset=tokenized_en_qum_test,\n",
    "    tokenizer=en_qum_tokenizer,\n",
    ")\n",
    "\n",
    "trainer_en_qum.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc2QH1HJcuKn"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9jCIC05CCG_"
   },
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "ab38943332ff43e8b3007af877d903ba",
      "da0e0ffe286d4770ba01aca5a77ee800",
      "98c08ea6eb6b462d8634b7c19c48a1e0",
      "e658e3852e994df78ea4f4b71cf7367e",
      "d746bcfa1d144e5899f65ee8819559f4",
      "f4972c6b951b47ad9b9d4401b81a9b00",
      "42b3ea9694574cbb890d2cba1e9da398",
      "df78d2c189004b9088a3e06cc8fd24ce",
      "cba9732bbd4a45069c23fdf862b58006",
      "391e7716fc8d4c6f80849b3726dae671",
      "e60f47e322044c3b811de6e854f92718",
      "e8900c5b8738432184d5e7e6b5874b92",
      "9e8d0c38e92049baad0b1ede5a8df2ad",
      "b9752081f6bd48ecb9fe0d006f4d6dfb",
      "9df27ed1ee604ccc913e8293490273a1",
      "42b44809e5424898b8964ca5e9086221",
      "914d4b65e4904d978f0019b76bd72f0d",
      "a21e97f3f46449cb85c2820aee17d0a8",
      "ad171ddc5edf43c6ae6b2b637f58a178",
      "882dc26f8ede4c989c7adb2d160c43fa",
      "0b338244a9264150a5451e2971bf9601",
      "480cb1d012744e5aa6b723b2dec8efe3",
      "66fdaf39cc7f42c790e4169951508b6c",
      "98bd79be14904fbfbeddb2ae704e9e63",
      "0864f37e6c994644a332f368c9fb7e83",
      "35762c1a7a484939a563d22d3361a33d",
      "e07b92ffead240e4a4fb9c29381b7dbe",
      "7d975643ffea4440a911d31edc963d47",
      "65ef9cce09854a919fcd248b8f6dfde0",
      "81b52971770348aaa254578324283072",
      "ce6043f05dc14315bb368be3650df059",
      "b7fd6d604dc942f0b477913d1e61e371",
      "0393d20388a44647ae42811e790bbacc",
      "71a69312b1bf43cf86a9c52b89cea77e",
      "cc9970dce18540e7af12037c261ed8d6",
      "478958b45a4f4328b896f8b0e31f2261",
      "eb9c5c887f884a74805b74e8f59e0f97",
      "d2580eb0efe346598c2541e4bd24ccc0",
      "9166840359bd41eebc29fd80a66e3ca1",
      "b6a5de51e430470d91587107cae482db",
      "a9343ded2e864a5d8fbce9f4fcce0595",
      "101eade1f1754cafb2e05e0505555f81",
      "365270de745144df9f8b859507410192",
      "ca3839fb84f14c0a879eeef649b590b9",
      "aca5dea9030e4ab09ed1439aaf227d6d",
      "3534e44601154adf9b7fc070d5899623",
      "81acb04524f241219223ac41035b2f54",
      "7bbcca56c93949058852ca12db5b19c3",
      "80a39bba1fe7416fbc3559da1ca8e154",
      "91ba10ff56744af49efa25f3dfc5967f",
      "ef9f2d82df7e462b80ac9749cfbb0784",
      "ed0c71b7c3e34a8290cace81c8ff1296",
      "6119e3e4ea7b446dbc2ee73c0f7eb081",
      "c1f20f52cb224b369c54dd95070818f1",
      "ef19bdf26ab54aa5bf9b59ee1aaf469b"
     ]
    },
    "id": "YEoaaYAZCDnX",
    "outputId": "2704be83-4707-451c-f310-1d32ff797201"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab38943332ff43e8b3007af877d903ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8900c5b8738432184d5e7e6b5874b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fdaf39cc7f42c790e4169951508b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a69312b1bf43cf86a9c52b89cea77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca5dea9030e4ab09ed1439aaf227d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coKh9lECcw45"
   },
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "m7IRRMdMiMtN"
   },
   "outputs": [],
   "source": [
    "origin_lang = \"deu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjB3SBkqn1JG"
   },
   "source": [
    "#### 1. Con Finetuning Español-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uBCVbR3obZG",
    "outputId": "69552b28-b73c-45c2-c444-f907342c2409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Traduciendo: deu → Español → Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [10:40<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_intermediate = []\n",
    "references_intermediate = []\n",
    "\n",
    "print(f\"\\n🔹 Traduciendo: {origin_lang} → Español → Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    # Paso 1: origin_lang → español\n",
    "    inter = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"spa_Latn\")\n",
    "\n",
    "    # Paso 2: español → \"Sipakapense\"\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, inter, \"spa_Latn\", \"qum\")\n",
    "\n",
    "    predictions_intermediate.append(pred)\n",
    "    references_intermediate.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qVv0v6pUvm-e"
   },
   "outputs": [],
   "source": [
    "clean_pred_intermediate = [pred.removeprefix(\"qum\").strip() for pred in predictions_intermediate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2roPLoJoy2L",
    "outputId": "b146ddba-6bcf-4bc3-bfd1-58cf5c97c0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' Frau benutzt nur gutes Holz.\n",
      "> Referencia:  wu rxqiil taʼ xuux xaq utz laj siʼ kchuknik chre\n",
      "> Predicción:  ri rkaʼyʼaqbʼik\n",
      "\n",
      "> Entrada:     #qum# Die ganze Pflaume ist weg.\n",
      "> Referencia:  Njeel ri xkarawaʼn xpochʼik rech chenim xchaqʼjik\n",
      "> Predicción:  ri ajkʼal riij riij xtzʼalxik ri rxbʼal\n",
      "\n",
      "> Entrada:     #qum# In unserem Dorf wird unsere Sprache nicht mehr gesprochen\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolbʼaal\n",
      "> Predicción:  , i'm not talking in my neighborhood\n",
      "\n",
      "> Entrada:     #qum# Der Schüler lernt, wie\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri suʼ\n",
      "> Predicción:  xik ri rkaʼy ri rkʼalxik ri rkaʼy\n",
      "\n",
      "> Entrada:     #qum# Die Frau war von ihrem Mann umarmt.\n",
      "> Referencia:  Ri ixoq tzʼulmaj rum ri rchjiil\n",
      "> Predicción:  . The woman was hugged by her husband.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicción:  {clean_pred_intermediate[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcM-8kilozFz",
    "outputId": "45fb1a55-d674-4156-afd8-e0d70df1f017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas BLEU ===\n",
      "BLEU-1: 2.79\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== Métricas ROUGE ===\n",
      "ROUGE-1:   6.67\n",
      "ROUGE-2:   0.21\n",
      "ROUGE-L:   6.51\n",
      "ROUGE-Lsum:6.49\n",
      "\n",
      "=== Otras Métricas ===\n",
      "METEOR:     3.30\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_intermediate, references=[[ref] for ref in references_intermediate])\n",
    "\n",
    "print(\"\\n=== Métricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Métricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Otras Métricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MTsr6mJBWrm",
    "outputId": "c0eb577a-3a40-4fde-b250-28ba31c62a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\n"
     ]
    }
   ],
   "source": [
    "with open(f\"results_{lang}_qum_finetune_intermediate.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_intermediate, references_intermediate):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(\"✅ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_UFY-dToFsb"
   },
   "source": [
    "#### 2. Con Finetuning Alemán-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCLhcjT-aUtA",
    "outputId": "54d0b36e-335a-4d85-c522-e86f7e843b76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qlora_finetuned_model_deu_quc_nllb/tokenizer_config.json',\n",
       " './qlora_finetuned_model_deu_quc_nllb/special_tokens_map.json',\n",
       " './qlora_finetuned_model_deu_quc_nllb/sentencepiece.bpe.model',\n",
       " './qlora_finetuned_model_deu_quc_nllb/added_tokens.json',\n",
       " './qlora_finetuned_model_deu_quc_nllb/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_model = en_qum_model\n",
    "finetune_tokenizer = en_qum_tokenizer\n",
    "\n",
    "finetune_model.save_pretrained(\"./qlora_finetuned_model_deu_quc_nllb\")\n",
    "finetune_tokenizer.save_pretrained(\"./qlora_finetuned_model_deu_quc_nllb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0UrnNGqobuo",
    "outputId": "2bbecc07-e556-45aa-bd4e-ac290d17e0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Traduciendo: deu → Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [08:39<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_direct = []\n",
    "references_direct = []\n",
    "\n",
    "print(f\"\\n🔹 Traduciendo: {origin_lang} → Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"qum\")\n",
    "\n",
    "    predictions_direct.append(pred)\n",
    "    references_direct.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chF42cMCu0k2"
   },
   "outputs": [],
   "source": [
    "clean_pred_direct = [pred.removeprefix(\"qum\").strip() for pred in predictions_direct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGGj_XKctmn9",
    "outputId": "9355892b-6a24-47b0-fd56-9f5ae1a7038c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' Frau benutzt nur gutes Holz.\n",
      "> Referencia:  wu rxqiil taʼ xuux xaq utz laj siʼ kchuknik chre\n",
      "> Predicción:  , and the other side is a right-hander than a right-hander.\n",
      "\n",
      "> Entrada:     #qum# Die ganze Pflaume ist weg.\n",
      "> Referencia:  Njeel ri xkarawaʼn xpochʼik rech chenim xchaqʼjik\n",
      "> Predicción:  's all that is left of the earth.\n",
      "\n",
      "> Entrada:     #qum# In unserem Dorf wird unsere Sprache nicht mehr gesprochen\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolbʼaal\n",
      "> Predicción:  in our village our language is no longer spoken\n",
      "\n",
      "> Entrada:     #qum# Der Schüler lernt, wie\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri suʼ\n",
      "> Predicción:  ate, and the student learns how to\n",
      "\n",
      "> Entrada:     #qum# Die Frau war von ihrem Mann umarmt.\n",
      "> Referencia:  Ri ixoq tzʼulmaj rum ri rchjiil\n",
      "> Predicción:  , the woman was in the arms of her husband.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicción:  {clean_pred_direct[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNuiAUOytvUu",
    "outputId": "284606a3-b7f1-4dc7-c024-e5a62fb59f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas BLEU ===\n",
      "BLEU-1: 0.00\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== Métricas ROUGE ===\n",
      "ROUGE-1:   0.34\n",
      "ROUGE-2:   0.00\n",
      "ROUGE-L:   0.33\n",
      "ROUGE-Lsum:0.34\n",
      "\n",
      "=== Otras Métricas ===\n",
      "METEOR:     0.02\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_direct, references=[[ref] for ref in references_direct])\n",
    "\n",
    "print(\"\\n=== Métricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Métricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Otras Métricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2o0_ZKITBrRf",
    "outputId": "ac75b503-2dae-4d53-a3ae-959903a290dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resultados guardados en 'results_de_qum_finetune_direct.tsv'\n"
     ]
    }
   ],
   "source": [
    "filename = f\"results_{lang}_qum_finetune_direct.tsv\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_direct, references_direct):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(f\"✅ Resultados guardados en '{filename}'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g5rb6MT0A5mD",
    "R6AMhN2GZpNt",
    "Q5ukIdzqYhCo",
    "o6ngW-l5c1aq",
    "qjWCp3bkc27u",
    "dazLUZlhKKHk",
    "tc2QH1HJcuKn",
    "U9jCIC05CCG_",
    "coKh9lECcw45",
    "ljNvMeB3ezyA",
    "rjB3SBkqn1JG",
    "y_UFY-dToFsb",
    "fCaQTajaoPyr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
