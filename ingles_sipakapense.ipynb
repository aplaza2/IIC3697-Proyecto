{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96PapqzwAUkB"
   },
   "source": [
    "# Experimentos de Traducción Inglés -> Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_FHD9kw266t",
    "outputId": "69a8c951-cbb9-443c-b074-99720455b3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yheiPHx8YN0m"
   },
   "outputs": [],
   "source": [
    "lang = \"en\"\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5rb6MT0A5mD"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yFaCwcFLx9HV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hg_qHsB95EzQ"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NF14LcemARSd"
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate peft transformers sentencepiece datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7-HRnAKmA8LR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8Nds_8Y1Mm"
   },
   "source": [
    "## Datasets\n",
    "!REQUIERE SUBIR LOS ARCHIVOS DEL DATASET SINTETICO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smVHNrAeY5ck"
   },
   "source": [
    "Dataset se puede encontrar en el [siguiente enlace](https://github.com/transducens/mayanv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1ViYHauY4v3",
    "outputId": "48f3c925-03a2-4c0e-c79a-4754d18d2ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mayanv'...\n",
      "remote: Enumerating objects: 177, done.\u001b[K\n",
      "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
      "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
      "remote: Total 177 (delta 22), reused 157 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (177/177), 1.35 MiB | 3.79 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/transducens/mayanv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ghoF4-noMjaC"
   },
   "outputs": [],
   "source": [
    "def load_lines(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def generate_dataset(language, train_folder=\"train\", test_folder=\"test\", base_path=\"mayanv/MayanV\"):\n",
    "    # Rutas\n",
    "    train_lang_path = f\"{base_path}/{language}/{train_folder}/data.{language}\"\n",
    "    test_lang_path = f\"{base_path}/{language}/{test_folder}/data.{language}\"\n",
    "\n",
    "    train_es_path = f\"{base_path}/{language}/{train_folder}/data.es\"\n",
    "    test_es_path = f\"{base_path}/{language}/{test_folder}/data.es\"\n",
    "\n",
    "    train_src = load_lines(train_lang_path)\n",
    "    train_tgt = load_lines(train_es_path)\n",
    "\n",
    "    test_src = load_lines(test_lang_path)\n",
    "    test_tgt = load_lines(test_es_path)\n",
    "\n",
    "    # Crea datasets\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_src, \"target\": train_tgt})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_src, \"target\": test_tgt})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PmYZbWQ1kPpQ"
   },
   "outputs": [],
   "source": [
    "def invertir_input_target(example):\n",
    "    return {\n",
    "        \"input\": example[\"target\"],\n",
    "        \"target\": example[\"input\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "80463f537d6a4a06a048707862bc3757",
      "b6627efd17ec4af280c234fa40de518e",
      "7a92681906ca4217a6c6fe765398f703",
      "103cced007154ac9890c1623463c0744",
      "3d93d8443fb1431db1beacc06dd423e2",
      "a457124b1ac741f7b7583febe7401165",
      "96e3917db4264d0ab99043276e7b0ca1",
      "13bb02e875de4d2cbfac13700ecfbf05",
      "8157b2e1038543fc826a5b1f93be7975",
      "5251f1499cfd42a3aab538eef9a2caa0",
      "78f10c752dc3493c8770992d8814b54d",
      "7e51348a745344329496d86102db0f4a",
      "4a75012ce7754e89be28533ecbd030ae",
      "a16868c0f44849908d214e7173cc557a",
      "16fe00b3d29643b0b45c14ce2dbe6fc9",
      "54f62b5d853145e68f340e58f029d6b6",
      "9ac1818be10b4b539745a18759318289",
      "86fc7f13bae04d71bd03f42310795629",
      "01724037e4764e608880ed97a457738d",
      "f936cad5b43b40c493e0e1b76d2b0bb3",
      "6111bbe1a281402b9652655933964641",
      "4d7138e980c14f7983a7280089e4692d"
     ]
    },
    "id": "4l0xcZcOgsmx",
    "outputId": "ecb06aa3-67ad-46e2-ecba-e5b6c83a6082"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80463f537d6a4a06a048707862bc3757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e51348a745344329496d86102db0f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_qum_dataset = generate_dataset(\"qum\", train_folder=\"test\", test_folder=\"dev\")\n",
    "es_qum_dataset = DatasetDict({\n",
    "    \"train\": es_qum_dataset[\"train\"].map(invertir_input_target),\n",
    "    \"test\": es_qum_dataset[\"test\"].map(invertir_input_target),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e-p1Rq94bTJL"
   },
   "outputs": [],
   "source": [
    "def generate_en_qum_dataset_from_files(\n",
    "    data_dir=\"./\",\n",
    "    base_path=\"mayanv/MayanV\",\n",
    "    language=\"qum\",\n",
    "    train_folder=\"test\",\n",
    "    test_folder=\"dev\",\n",
    "    lang=\"en\"\n",
    "):\n",
    "    # Paths a los archivos .en\n",
    "    train_en_path = os.path.join(data_dir, f\"train.{lang}\")\n",
    "    test_en_path = os.path.join(data_dir, f\"test.{lang}\")\n",
    "\n",
    "    # Paths a los archivos .qum\n",
    "    train_qum_path = os.path.join(base_path, language, train_folder, \"data.qum\")\n",
    "    test_qum_path = os.path.join(base_path, language, test_folder, \"data.qum\")\n",
    "\n",
    "    # Carga las líneas\n",
    "    train_en = load_lines(train_en_path)\n",
    "    test_en = load_lines(test_en_path)\n",
    "\n",
    "    train_qum = load_lines(train_qum_path)\n",
    "    test_qum = load_lines(test_qum_path)\n",
    "\n",
    "    # Validación\n",
    "    assert len(train_en) == len(train_qum), f\"Train mismatch: {len(train_en)} vs {len(train_qum)}\"\n",
    "    assert len(test_en) == len(test_qum), f\"Test mismatch: {len(test_en)} vs {len(test_qum)}\"\n",
    "\n",
    "    # Combina en datasets Hugging Face\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_en, \"target\": train_qum})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_en, \"target\": test_qum})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })\n",
    "\n",
    "\n",
    "en_qum_dataset = generate_en_qum_dataset_from_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6AMhN2GZpNt"
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4fo2s-t9Zozn"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 128\n",
    "\n",
    "    inputs = finetune_tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    targets = finetune_tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def translate_text(model, tokenizer, text, src_lang, tgt_lang):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    forced_bos_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "    # Detecta el dispositivo (GPU o CPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokeniza y mueve a la misma device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Generación\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def build_and_save_split_en_only(split, model, tokenizer, output_dir):\n",
    "    input_path = os.path.join(output_dir, f\"{split}.{lang}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"w\", encoding=\"utf-8\") as f_en:\n",
    "        print(f\"🔄 Traduciendo y guardando split '{split}' al {lang}...\\n\")\n",
    "\n",
    "        for example in tqdm(es_qum_dataset[split]):\n",
    "            es_text = example[\"input\"].replace(\"#qum#\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                en_text = translate_text(model, tokenizer, es_text, \"spa_Latn\", f\"{lang}_Latn\")\n",
    "                en_text = \"#qum# \" + en_text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error traduciendo: {es_text}\\n{e}\")\n",
    "                en_text = \"#qum#\"\n",
    "\n",
    "            f_en.write(en_text + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Archivo '{split}.{lang}' guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5ukIdzqYhCo"
   },
   "source": [
    "## Crear dataset sintético\n",
    "!SOLO EJECUTAR SI NO SE TIENE EL DATASET TODAVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOjTDQjmYq8F"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "mid_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "mid_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "mid_tokenizer.src_lang = \"spa_Latn\"\n",
    "mid_forced_bos_id = mid_tokenizer.convert_tokens_to_ids(f\"{lang}_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U4gFjfFYots"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/en_qum_dataset\"\n",
    "\n",
    "build_and_save_split_en_only(\"test\", mid_model, mid_tokenizer, output_dir)\n",
    "build_and_save_split_en_only(\"train\", mid_model, mid_tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6y8Xh2RBAmS"
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWCp3bkc27u"
   },
   "source": [
    "### 1. Modelo con Finetuning Español-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "ed877ff6c771498b9c4d4928b5d7370f",
      "0f2872267f6b4bb3949f5fe2c83a5124",
      "5d558e203e3845d6974272bf1d9e1444",
      "f278417c2a40449998da047a0e25dbed",
      "73e124a326f447209651635e370db6b3",
      "22263ede295b4401ba04c1174a0133ee",
      "3bf165cda1d64c36abf2e225021e90b8",
      "b314d5d9cf274abfb3b7c68571f3f569",
      "3878b48b639c455787a187417e3b4cd8",
      "ed09feeadd97448db1dbb332322d5f6b",
      "9629936fe3024a80b6ea88251d7482f3",
      "c822b4e131504977b14280b4317af59e",
      "2a7ca5fa265546d4a3e61f24126e8461",
      "9a8e90f9180a4b34895129b17eb651b4",
      "f3395bda08894134900e5d232d1e796c",
      "ca5b303c4dd34ca7836a1490495d6df2",
      "8dca799b59334d2b9e8485bccc7da715",
      "50f14c5c36574c2d802e3cb6dbb4bc38",
      "30ff4c2276fb40c28d2bac7b9425e6c3",
      "d79b459306034e4d84355fe6aeaae44e",
      "9a4e541a96cb42d2a31caf12d5762eb1",
      "e3f0e788deb04d96aeab571bb35c7a1c",
      "a5e85a0c308a4e629d968abeb5482284",
      "d7f6eecbece1450bb2b076690a14595e",
      "52108039dcb94d4893360a9ba04ed0a3",
      "f57e24108b81411f858266da992d0632",
      "5bc09a9160b14226a669e557482cd70d",
      "6128c3609d474933994a347112de1dd1",
      "55ae7d858b8d42c7b9675da71c1a1026",
      "acb938dad17e4f1fb8b368e433b59c43",
      "5e14838370424410adcda3e49c57a9b6",
      "586b1f710bc44b798b41ec6dc96caf04",
      "97139153faf9482e9617bcc013336d00",
      "c7c25faaf26746fe8a66c6d1b490b157",
      "d927be8f2a60486face0a307fb17bb2d",
      "31d39da77ca14e5390de0416ed712e97",
      "ecf339cd128d423a802622bee1301f06",
      "2378af72dd1148fe984d39a626c6ac8a",
      "a46a30b47ad74b258d7ff6e68fc29058",
      "5033ddfd501c4b5bb526f3e75b1f374a",
      "ce68cfbbe4224a03a468b022c62d5954",
      "502930d3cd494d3a90cd2d31e2b68028",
      "4c7c5d9ac2804287a0476d4b522926ac",
      "084f9f26c10f4acba77788edf4f7404a",
      "54b1d71a81c0436295230f9051e85c7c",
      "2437dfd7ae7d4ecd840d0db5ee11dcb7",
      "aa590872a7d24cc2a68960f66f1327c4",
      "4b52510a312045588f4dc7f45b5e43c7",
      "190d57b605c74448a76631b3d413e9df",
      "e768371d3eef4a9894ddb3a71fd6f1c9",
      "f4d4c5fc705847f7ae1efbcf565cf4c2",
      "1ca41738ec10498d834f5470f7ea16ca",
      "0e71eb92a89d4e99ae749bcf527f3d5d",
      "57b23e6747f649d994cdb32dec6ccbed",
      "b406a9537b6a46a3a7106c1cf92ca411",
      "c7d0ab4d5f5046cb999628b3482087dc",
      "07daae72b93040c6b0cb249468930f73",
      "ad1878f822ae4cbcb67f8c8c63873c2c",
      "34cb0278e3234d75b84e1818a4ad9f93",
      "dc3275aa4a864aec8bcaf42b4e9492bb",
      "f10a21c3193f42c981c8b64de8d1f4a3",
      "78520f9883924a588563c307fd42c085",
      "4aaa0644f0f94f7ca4d0f751150e861c",
      "a6c803f557844e92a164ce3130878945",
      "be4d73ce2b3740009eb22213b75b58ae",
      "d943bd39296141179447a4744f009748",
      "e1cb64460a5a4e0880e4755d3a2bb229",
      "d7b676b42d104c6cb9c164bcb6e6924b",
      "781b026636db43018125a51003550066",
      "55de7f8426e84234981e88ddf01e6456",
      "a6e1671709fd4516ac1117f2b6335e33",
      "e702e6611ef6497488e61bf231481c44",
      "e3100a3310004fc981462191c3da9550",
      "893a90ad46b3424fae28c2c8106f3dc7",
      "bb641e5cf86249b984cbea3b41c71e0b",
      "00b595a6346f420a8865f53db5fad71f",
      "3766d623849d4dfdbe4405ae6f9700c9",
      "f967ec86b83448399f49fefc26a77d85",
      "2204a25e409d43919c12bc4ab9e0f93f",
      "626fd8566d99485ab97a5547c5ad7bc4",
      "19238a7e399d4e50a00173a7b26fccb9",
      "8db726fd36e849cabc659a636189b85e",
      "b3aa4cc6fc1a4f719fb04b19f0a3b8bf",
      "cffac417c3224cacad0e6bb015db89cf",
      "48e38832e94e4c098b53e1711b169ed5",
      "df8f4dba0aea4a0f89eb12acba89702e",
      "d3f7338915d5469898e784b7aed2da77",
      "fb552e778bd64226b84f43b63c2c9fb9"
     ]
    },
    "id": "j0zKtLz_K7Uc",
    "outputId": "607e8d96-c6b1-4ce6-b6cf-d99462fa91b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed877ff6c771498b9c4d4928b5d7370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c822b4e131504977b14280b4317af59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e85a0c308a4e629d968abeb5482284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c25faaf26746fe8a66c6d1b490b157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b1d71a81c0436295230f9051e85c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d0ab4d5f5046cb999628b3482087dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cb64460a5a4e0880e4755d3a2bb229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f967ec86b83448399f49fefc26a77d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configurar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Aplicar QLoRA\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-TAeIyJ4pOZM",
    "outputId": "7893a043-5469-46f4-de52-337a3a857483"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Añadir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "finetune_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "finetune_model.resize_token_embeddings(len(finetune_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "85d777596e0341f39702b87d649840cd",
      "4e70477523c143cb820b4d06e3d99674",
      "5486617a07b64d688b3851f2ff677e68",
      "dba23a24ab0b4a1a84cca5fbf5b31bdc",
      "a6761e18db404120adc3645c6bb5ed1a",
      "86960611c9ff4a5ab9edaa5995a7f9a9",
      "40e08433524b44adbf0e63a4c1c4ef86",
      "b0894800f292473998691518377e3482",
      "c6f0f7a23d5d46cfba7eb22b7d0026eb",
      "757b5401e41f41348fd90f23b7e5d5f9",
      "4fa6595d6d2b4c3a8850cd9dd3d3ba6f",
      "a9c0fb5caed940d29e3921327b891689",
      "bb151affdaf4475983a456e963354811",
      "e4299e189a4b44d185b1a277f56f41b7",
      "a98d657cd1a14712bf9ac83e90586887",
      "c75b8622b2194685a920682c3aed9cbf",
      "186fc1076cf142e381d4d9375bda2a46",
      "2a6d63af6f83489a9523d7b01b02e971",
      "cb95f4c7c8cb40f786a1ad920ace432c",
      "a7ec6bc278cd4e998a9ccf30fa9fbda6",
      "04c042fb5d9a4db2a779da5d94ed94d9",
      "2a7ea1a7411a454b85bcf6967194bb0c"
     ]
    },
    "id": "R2iPhpoQnSc2",
    "outputId": "7f317c9e-5dbd-4f71-95c7-eb179b9dcc8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d777596e0341f39702b87d649840cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c0fb5caed940d29e3921327b891689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = es_qum_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = es_qum_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aXgDM4lAnagN",
    "outputId": "593c3d88-467f-4875-f199-d8dbd699e570"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-15-1857230931.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 19:55, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>7.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>7.028200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qlora_finetuned_qum_nllb/tokenizer_config.json',\n",
       " './qlora_finetuned_qum_nllb/special_tokens_map.json',\n",
       " './qlora_finetuned_qum_nllb/sentencepiece.bpe.model',\n",
       " './qlora_finetuned_qum_nllb/added_tokens.json',\n",
       " './qlora_finetuned_qum_nllb/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_finetuned_qum_nllb\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_qlora\",\n",
    "    save_total_limit=2,\n",
    "    bf16=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=finetune_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "finetune_model.save_pretrained(\"./qlora_finetuned_qum_nllb\")\n",
    "finetune_tokenizer.save_pretrained(\"./qlora_finetuned_qum_nllb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dazLUZlhKKHk"
   },
   "source": [
    "### 2. Modelo con Finetuning Inglés-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LEUkIjpj33ln"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "en_qum_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "en_qum_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Stma1J90dTmA",
    "outputId": "5b52599e-8e73-4871-d177-6926ee30d755"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Añadir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "en_qum_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "en_qum_model.resize_token_embeddings(len(en_qum_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "ee43615cffd14804980d4b340e98db60",
      "bb82583d438d459db6347665bf0ac8dd",
      "20f7bd3b474c4a6a81a3e9249e665b24",
      "bbdae53d922c4af9aa532666024c5387",
      "27f43b4642394d80aaebbed549c3c01c",
      "dba087e13103408d94eb1739711d317b",
      "222d377e6b9f4a1298cf77fd6361ee76",
      "990de8c6945f448c92c97ce7b33fe745",
      "b8dd4175e56b468fbe6425843ca1ab1d",
      "605629a82a1e452faabb965280b560cc",
      "604f3f980aa941ba86ce0b40c9068efb",
      "d26d7f64c30441838715e0e24a0b11ee",
      "0e07f4ae534a43318e03780fb073348a",
      "6879968329554dca8e1660078aaa8cef",
      "f835c3ec7d9b429395638a3cf433b638",
      "e9658fa4e19d48919a4b67255ff49476",
      "2185d0e4f2804dbb97857da7c1c69418",
      "46504dde9f9c45eaaabb8a29b5c480f2",
      "e37b327e8956446bb40d550d7857912d",
      "c771ca1f84e647818d7125ba42f0848c",
      "2c935798f9044e63ab806ce298d9611e",
      "96a41dcc9dd3489bbdea057b9c48828b"
     ]
    },
    "id": "w1sEWghP4B7C",
    "outputId": "e851132d-6529-44ef-eb70-de6bdc159e58"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee43615cffd14804980d4b340e98db60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26d7f64c30441838715e0e24a0b11ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function_en_qum(examples):\n",
    "    max_length = 128\n",
    "    inputs = en_qum_tokenizer(examples[\"input\"], max_length=max_length, truncation=True, padding=True)\n",
    "    targets = en_qum_tokenizer(examples[\"target\"], max_length=max_length, truncation=True, padding=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_en_qum_train = en_qum_dataset[\"train\"].map(preprocess_function_en_qum, batched=True)\n",
    "tokenized_en_qum_test = en_qum_dataset[\"test\"].map(preprocess_function_en_qum, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxivqDvt4GEf",
    "outputId": "244c1639-0343-4f9a-f0f4-101e374a64b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args_en_qum = TrainingArguments(\n",
    "    output_dir=\"./finetuned_en_qum_nllb\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=n_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_en_qum\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WWYUxQWm4IU_",
    "outputId": "986e29e5-539b-431e-8590-61dd0cbb0c83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-27-1310942191.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_en_qum = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 21:00, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>8.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.638200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>6.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>6.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>6.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>6.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>6.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>6.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>6.222500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=6.957568001302083, metrics={'train_runtime': 1260.9977, 'train_samples_per_second': 11.895, 'train_steps_per_second': 1.487, 'total_flos': 764420751360000.0, 'train_loss': 6.957568001302083, 'epoch': 15.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_en_qum = Trainer(\n",
    "    model=en_qum_model,\n",
    "    args=training_args_en_qum,\n",
    "    train_dataset=tokenized_en_qum_train,\n",
    "    eval_dataset=tokenized_en_qum_test,\n",
    "    tokenizer=en_qum_tokenizer,\n",
    ")\n",
    "\n",
    "trainer_en_qum.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc2QH1HJcuKn"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9jCIC05CCG_"
   },
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "f609c2a5fd8d4811b9a1b1eea5208ac7",
      "809ab54ec9df4fff83debd5e3e3e4eb8",
      "1f50e2636dc040dcaa69bc86d9bc4586",
      "e2ff98d3f5f7481fa7e05fcbaba7bd64",
      "2ffdca8e8b4549c4bd3ce0b88d7dbb53",
      "e61cdc75f73e43f3aaa1c5d9cec3acd7",
      "b96c84ab2b72440fbae7f16fc5e685f5",
      "9a8837b735c54778965b561d89681536",
      "5c2affd570ca497188eeda66ff81009e",
      "a077e5853bd64ed18c19b653a8d3d2a1",
      "555fd415bf804ef5a2d07e929d46637a",
      "3417816cd0974a8aab3997af38026024",
      "8b47c9808eba46aa908395a80d17a7ae",
      "9c6854feab25455198d5c875a79b1687",
      "9d753d7f2a0c47c59c8f1ab089dcc492",
      "8d1dc623aa274b35b480964b781bef55",
      "ac17d1d4bfcb4f4a9c56813ba00790ab",
      "8bae3ca3fc2842b9804774810f1b65ac",
      "91307ceea1b04b1e80b128d0ba6a5198",
      "7904483626ae4a70b16562784f1101cc",
      "16b94914d720416987e4088c4712b75f",
      "757eb084656e43a2a9ef1e8a0f732057",
      "ed307e1d8f734648bc164f8018899520",
      "307004224d8b416f95897d9c9bbf7e3d",
      "552b8b2dac414f8c92382f5d8f88eb31",
      "6cd2629ab45d4f6eb66211ecf7f278c4",
      "a47ff535606549fda0ce44f68ddbe716",
      "611172bdd8bc4fed8ce14b447d4b39ea",
      "0b76a3d19b2747c0916ad5ba9977e511",
      "6036bd42f6464ed9b18c46a570a497b3",
      "c3c2b6ee979b44d798cdc59db5a9f92b",
      "3fef9961b9704df38f6a108e942ad6c9",
      "38ed27ded66a4b08977d522c4d74d78c",
      "256e10e2549f41fda46c4cebc94d2982",
      "f624cdd4a93846b0b3dd070f2bc2cc28",
      "e88099cf2d2642188ee6c67db9b5f87f",
      "a119430b092046afa081fbf57bb35523",
      "bdf55349d0604c668bee198b113af5f2",
      "792f118835894f69b91ec4a068fbe80f",
      "1efe595d84d64b04aa973942d4245338",
      "fa62e80e428649f29b3dbef17b02b8a5",
      "efe4755ac9e943acbef2428607f1cd5b",
      "63be17e436724c13844ca07a7ad988ea",
      "ee72db4ade3e430eb7f1c8f55b196385",
      "f26caa06dc34404fbb9477d196b57950",
      "28a169369a35437ba06ed8782f5d3803",
      "c6a63d4bfcde4c2499afaf0a60f3dead",
      "7139585a415a410bb9ebd7787af3d5db",
      "feb7cefbe9b343b680bce77dcc9233ae",
      "b0e76f4901c44353abcb337a1f3ceecb",
      "27e9cc70fc2143daa7e959e8ffac4688",
      "3a0f74165ba541689b3f2fc312ccd281",
      "5cf1e764afdf4819b30d85862decf596",
      "62a3c3a56ef64ac8a290685e48be171a",
      "2bc818c6dfc044a2ae375dcfd60313bb"
     ]
    },
    "id": "YEoaaYAZCDnX",
    "outputId": "1f5810f9-72a2-484b-e243-30f538e3a0ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f609c2a5fd8d4811b9a1b1eea5208ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3417816cd0974a8aab3997af38026024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed307e1d8f734648bc164f8018899520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256e10e2549f41fda46c4cebc94d2982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26caa06dc34404fbb9477d196b57950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coKh9lECcw45"
   },
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "m7IRRMdMiMtN"
   },
   "outputs": [],
   "source": [
    "origin_lang = \"eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjB3SBkqn1JG"
   },
   "source": [
    "#### 1. Con Finetuning Español-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uBCVbR3obZG",
    "outputId": "556c2df6-3039-4fdd-f339-bfbb6530c4eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Traduciendo: eng → Español → Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [10:18<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_intermediate = []\n",
    "references_intermediate = []\n",
    "\n",
    "print(f\"\\n🔹 Traduciendo: {origin_lang} → Español → Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    # Paso 1: origin_lang → español\n",
    "    inter = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"spa_Latn\")\n",
    "\n",
    "    # Paso 2: español → \"Sipakapense\"\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, inter, \"spa_Latn\", \"qum\")\n",
    "\n",
    "    predictions_intermediate.append(pred)\n",
    "    references_intermediate.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qVv0v6pUvm-e"
   },
   "outputs": [],
   "source": [
    "clean_pred_intermediate = [pred.removeprefix(\"qum\").strip() for pred in predictions_intermediate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2roPLoJoy2L",
    "outputId": "12939517-55db-47bb-9f4f-1ef4480ba4be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' wife only uses good wood.\n",
      "> Referencia:  wu rxqiil taʼ xuux xaq utz laj siʼ kchuknik chre\n",
      "> Predicción:  ri ajkʼalobʼ ri rkʼoʼq\n",
      "\n",
      "> Entrada:     #qum# All the peaches are\n",
      "> Referencia:  Njeel ri xkarawaʼn xpochʼik rech chenim xchaqʼjik\n",
      "> Predicción:  q qʼiij ri rqʼiij ri rkʼiij\n",
      "\n",
      "> Entrada:     #qum# Our people don't speak our language anymore.\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolbʼaal\n",
      "> Predicción:  ri rwiʼjbʼal ri xbʼal\n",
      "\n",
      "> Entrada:     #qum# The student is learning how\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri suʼ\n",
      "> Predicción:  rxʼiyik rqʼoʼqʼik\n",
      "\n",
      "> Entrada:     #qum# The woman was hugged by her husband\n",
      "> Referencia:  Ri ixoq tzʼulmaj rum ri rchjiil\n",
      "> Predicción:  ri rwiʼyʼy ri rbʼan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicción:  {clean_pred_intermediate[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcM-8kilozFz",
    "outputId": "ffd03a26-2485-4dba-aabc-1e4c5d1d4b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas BLEU ===\n",
      "BLEU-1: 15.97\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== Métricas ROUGE ===\n",
      "ROUGE-1:   15.48\n",
      "ROUGE-2:   0.40\n",
      "ROUGE-L:   15.10\n",
      "ROUGE-Lsum:15.01\n",
      "\n",
      "=== Otras Métricas ===\n",
      "METEOR:     8.15\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_intermediate, references=[[ref] for ref in references_intermediate])\n",
    "\n",
    "print(\"\\n=== Métricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Métricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Otras Métricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MTsr6mJBWrm",
    "outputId": "afa8c30e-8b2d-409c-8866-2cf20af55efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\n"
     ]
    }
   ],
   "source": [
    "with open(f\"results_{lang}_qum_finetune_intermediate.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_intermediate, references_intermediate):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(\"✅ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_UFY-dToFsb"
   },
   "source": [
    "#### 2. Con Finetuning Inglés-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0UrnNGqobuo",
    "outputId": "309c4b76-1a86-43d8-ef33-945825c39444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Traduciendo: eng → Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [04:28<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_direct = []\n",
    "references_direct = []\n",
    "\n",
    "print(f\"\\n🔹 Traduciendo: {origin_lang} → Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"qum\")\n",
    "\n",
    "    predictions_direct.append(pred)\n",
    "    references_direct.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "chF42cMCu0k2"
   },
   "outputs": [],
   "source": [
    "clean_pred_direct = [pred.removeprefix(\"qum\").strip() for pred in predictions_direct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGGj_XKctmn9",
    "outputId": "35bbd1e8-7983-4783-c696-a7ec9d40e1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' wife only uses good wood.\n",
      "> Referencia:  wu rxqiil taʼ xuux xaq utz laj siʼ kchuknik chre\n",
      "> Predicción:  ri ajkʼaʼl ri xkʼiʼj\n",
      "\n",
      "> Entrada:     #qum# All the peaches are\n",
      "> Referencia:  Njeel ri xkarawaʼn xpochʼik rech chenim xchaqʼjik\n",
      "> Predicción:  ri ajkʼoom\n",
      "\n",
      "> Entrada:     #qum# Our people don't speak our language anymore.\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolbʼaal\n",
      "> Predicción:  ri rkaʼbʼik ri achii xtzʼaqbʼik\n",
      "\n",
      "> Entrada:     #qum# The student is learning how\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri suʼ\n",
      "> Predicción:  ri ajkʼalbʼal wu rkaʼyik\n",
      "\n",
      "> Entrada:     #qum# The woman was hugged by her husband\n",
      "> Referencia:  Ri ixoq tzʼulmaj rum ri rchjiil\n",
      "> Predicción:  ri achii xqʼiʼbʼal ri ajkʼalobʼ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicción:  {clean_pred_direct[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNuiAUOytvUu",
    "outputId": "f81566cd-04ee-4aa0-dde3-3560a6d26c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas BLEU ===\n",
      "BLEU-1: 16.64\n",
      "BLEU-2: 0.20\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== Métricas ROUGE ===\n",
      "ROUGE-1:   14.83\n",
      "ROUGE-2:   0.59\n",
      "ROUGE-L:   14.21\n",
      "ROUGE-Lsum:14.19\n",
      "\n",
      "=== Otras Métricas ===\n",
      "METEOR:     7.44\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_direct, references=[[ref] for ref in references_direct])\n",
    "\n",
    "print(\"\\n=== Métricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Métricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Otras Métricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2o0_ZKITBrRf",
    "outputId": "f658e952-a809-4580-b298-3fac6a983ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resultados guardados en 'results_en_qum_finetune_direct.tsv'\n"
     ]
    }
   ],
   "source": [
    "filename = f\"results_{lang}_qum_finetune_direct.tsv\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_direct, references_direct):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(f\"✅ Resultados guardados en '{filename}'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g5rb6MT0A5mD",
    "R6AMhN2GZpNt",
    "Q5ukIdzqYhCo",
    "o6ngW-l5c1aq",
    "qjWCp3bkc27u",
    "dazLUZlhKKHk",
    "tc2QH1HJcuKn",
    "U9jCIC05CCG_",
    "coKh9lECcw45",
    "ljNvMeB3ezyA",
    "rjB3SBkqn1JG",
    "y_UFY-dToFsb",
    "fCaQTajaoPyr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
