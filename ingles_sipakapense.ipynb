{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96PapqzwAUkB"
   },
   "source": [
    "# Experimentos de Traducci√≥n Ingl√©s -> Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_FHD9kw266t",
    "outputId": "69a8c951-cbb9-443c-b074-99720455b3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yheiPHx8YN0m"
   },
   "outputs": [],
   "source": [
    "lang = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5rb6MT0A5mD"
   },
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yFaCwcFLx9HV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Hg_qHsB95EzQ"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NF14LcemARSd"
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate peft transformers sentencepiece datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7-HRnAKmA8LR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8Nds_8Y1Mm"
   },
   "source": [
    "## Datasets\n",
    "!REQUIERE SUBIR LOS ARCHIVOS DEL DATASET SINTETICO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smVHNrAeY5ck"
   },
   "source": [
    "Dataset se puede encontrar en el [siguiente enlace](https://github.com/transducens/mayanv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1ViYHauY4v3",
    "outputId": "946c82b0-de5b-4351-debd-9ddb01a3db4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mayanv' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/transducens/mayanv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ghoF4-noMjaC"
   },
   "outputs": [],
   "source": [
    "def load_lines(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def generate_dataset(language, train_folder=\"train\", test_folder=\"test\", base_path=\"mayanv/MayanV\"):\n",
    "    # Rutas\n",
    "    train_lang_path = f\"{base_path}/{language}/{train_folder}/data.{language}\"\n",
    "    test_lang_path = f\"{base_path}/{language}/{test_folder}/data.{language}\"\n",
    "\n",
    "    train_es_path = f\"{base_path}/{language}/{train_folder}/data.es\"\n",
    "    test_es_path = f\"{base_path}/{language}/{test_folder}/data.es\"\n",
    "\n",
    "    train_src = load_lines(train_lang_path)\n",
    "    train_tgt = load_lines(train_es_path)\n",
    "\n",
    "    test_src = load_lines(test_lang_path)\n",
    "    test_tgt = load_lines(test_es_path)\n",
    "\n",
    "    # Crea datasets\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_src, \"target\": train_tgt})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_src, \"target\": test_tgt})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PmYZbWQ1kPpQ"
   },
   "outputs": [],
   "source": [
    "def invertir_input_target(example):\n",
    "    return {\n",
    "        \"input\": example[\"target\"],\n",
    "        \"target\": example[\"input\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "13f20b199c924755aca8d3a1aabd07a7",
      "aae6e44a75584a3193280d9bb5d1f5bf",
      "4b8fa516036044a9b9ac024bec40a03a",
      "4560211cd7184561bcf0b7593d982e71",
      "e6305597114b49478ec3911665c00f4e",
      "8f0dc1b87f3c45608a4c31b2ed48900d",
      "3d980e6efbbc41258a08ce3c820c3b3f",
      "3a88c4a6376f4bcb94c844a6bad8292a",
      "3c3b995ff7794029a44c2045df2734b4",
      "16bb0c1cc2ee4aa6a5736a7c3bf5b090",
      "d0e91211e23344088501f2e68c3ac025",
      "397c49bb97a84695a104203dc3d6ecdb",
      "0750d75d0bd041a890961e8c1b832240",
      "b3fabd18813b4e648e4136d66e45725e",
      "3c107798cdcb4c599ecc61fb6a852d38",
      "b06a5046dae04d5991a6cd4214231aa4",
      "33aea3c580a84ff98fc7520859b8236a",
      "413a0035d2e845118e36cb308df8992f",
      "fc7526b4e3f84c5db1682336e65934f3",
      "5403f6e13490453aa65f98c40b9ccc80",
      "e89a9ac52aa947a8a62ae73ba4c28af0",
      "387a4d2666a747c3bf00dc1372e5d52c"
     ]
    },
    "id": "4l0xcZcOgsmx",
    "outputId": "71ca96b4-9828-4cf1-a54b-c7811c6990a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f20b199c924755aca8d3a1aabd07a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397c49bb97a84695a104203dc3d6ecdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_qum_dataset = generate_dataset(\"qum\", train_folder=\"test\", test_folder=\"dev\")\n",
    "es_qum_dataset = DatasetDict({\n",
    "    \"train\": es_qum_dataset[\"train\"].map(invertir_input_target),\n",
    "    \"test\": es_qum_dataset[\"test\"].map(invertir_input_target),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e-p1Rq94bTJL"
   },
   "outputs": [],
   "source": [
    "def generate_en_qum_dataset_from_files(\n",
    "    data_dir=\"./\",\n",
    "    base_path=\"mayanv/MayanV\",\n",
    "    language=\"qum\",\n",
    "    train_folder=\"test\",\n",
    "    test_folder=\"dev\",\n",
    "    lang=\"en\"\n",
    "):\n",
    "    # Paths a los archivos .en\n",
    "    train_en_path = os.path.join(data_dir, f\"train.{lang}\")\n",
    "    test_en_path = os.path.join(data_dir, f\"test.{lang}\")\n",
    "\n",
    "    # Paths a los archivos .qum\n",
    "    train_qum_path = os.path.join(base_path, language, train_folder, \"data.qum\")\n",
    "    test_qum_path = os.path.join(base_path, language, test_folder, \"data.qum\")\n",
    "\n",
    "    # Carga las l√≠neas\n",
    "    train_en = load_lines(train_en_path)\n",
    "    test_en = load_lines(test_en_path)\n",
    "\n",
    "    train_qum = load_lines(train_qum_path)\n",
    "    test_qum = load_lines(test_qum_path)\n",
    "\n",
    "    # Validaci√≥n\n",
    "    assert len(train_en) == len(train_qum), f\"Train mismatch: {len(train_en)} vs {len(train_qum)}\"\n",
    "    assert len(test_en) == len(test_qum), f\"Test mismatch: {len(test_en)} vs {len(test_qum)}\"\n",
    "\n",
    "    # Combina en datasets Hugging Face\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_en, \"target\": train_qum})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_en, \"target\": test_qum})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })\n",
    "\n",
    "\n",
    "en_qum_dataset = generate_en_qum_dataset_from_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6AMhN2GZpNt"
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4fo2s-t9Zozn"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 128\n",
    "\n",
    "    inputs = finetune_tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    targets = finetune_tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def translate_text(model, tokenizer, text, src_lang, tgt_lang):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    forced_bos_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "    # Detecta el dispositivo (GPU o CPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokeniza y mueve a la misma device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Generaci√≥n\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def build_and_save_split_en_only(split, model, tokenizer, output_dir):\n",
    "    input_path = os.path.join(output_dir, f\"{split}.{lang}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"w\", encoding=\"utf-8\") as f_en:\n",
    "        print(f\"üîÑ Traduciendo y guardando split '{split}' al {lang}...\\n\")\n",
    "\n",
    "        for example in tqdm(es_qum_dataset[split]):\n",
    "            es_text = example[\"input\"].replace(\"#qum#\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                en_text = translate_text(model, tokenizer, es_text, \"spa_Latn\", f\"{lang}_Latn\")\n",
    "                en_text = \"#qum# \" + en_text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error traduciendo: {es_text}\\n{e}\")\n",
    "                en_text = \"#qum#\"\n",
    "\n",
    "            f_en.write(en_text + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Archivo '{split}.{lang}' guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5ukIdzqYhCo"
   },
   "source": [
    "## Crear dataset sint√©tico\n",
    "!SOLO EJECUTAR SI NO SE TIENE EL DATASET TODAVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOjTDQjmYq8F"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "mid_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "mid_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "mid_tokenizer.src_lang = \"spa_Latn\"\n",
    "mid_forced_bos_id = mid_tokenizer.convert_tokens_to_ids(f\"{lang}_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U4gFjfFYots"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/en_qum_dataset\"\n",
    "\n",
    "build_and_save_split_en_only(\"test\", mid_model, mid_tokenizer, output_dir)\n",
    "build_and_save_split_en_only(\"train\", mid_model, mid_tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6y8Xh2RBAmS"
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWCp3bkc27u"
   },
   "source": [
    "### 1. Modelo con Finetuning Espa√±ol-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "449c4ec07cc24d1a9f037590accf2013",
      "8e9b24041c3a47d6826edc07db8648b6",
      "83c1408a70ad496dbe4818a5585c58e2",
      "8afdeeb4106448a2a6ac7a4c3214b957",
      "ff6cfd792e834d4c928aa4ae606a8187",
      "1b28b9ff27d042858c750f66106b749d",
      "c27c15308877416d925fc39e17ad2ca4",
      "cd326821e9604d2fa538a73ad8314740",
      "1d305fd846ee4e1d8c1f0652c63154c5",
      "f2d0a423462a41fe86c983d2a323d0c8",
      "18ee418df9744d75aa8001c3fc3616ac",
      "9ba9da72581f4538b9e91f097115c0b9",
      "ea4383238acf49bca525403b40854d70",
      "d5c65e44143946979f9b0af06f0ceafd",
      "46b7e5eed23c4d7d94feceb82d9058b7",
      "5daceadb899f45b29126683a13e0c58e",
      "71c6f332211746749a6c5ad359f1b9ec",
      "647f812aeade4ee5813e3a9c1ae2d25f",
      "9af9289b9ccd4d9cadfa356669b1cb79",
      "b93b4103a8ac4e7a8b9fc276d97a4bea",
      "fa6b07afa3c74942a3315dd173db1045",
      "c62ab22aaefa4949a7da1d430bbfbfc0",
      "af7ddfa0b36a4f27ad0ea37f27a71866",
      "85f2d015cad4411b86822a1dc2f28b3e",
      "5d2978f18a1449d393e96335247501ee",
      "031bc6edd44d43bf93fde1bb0c7e9aa4",
      "2f0affccf9d04ccf86ea385b03adcea2",
      "b3b338d4762d467fbbc665e870bd4830",
      "be327a96b57644fa85126554ef50f5d3",
      "cc2fc372ab354f2db36dfda4bb32dda8",
      "4a19379ac1d14bf9bc1cf4c103abb051",
      "7f64475c63df4b599495ac33383fa443",
      "6a92a0335cac4bfc9153d02b017cb38b",
      "9d20a8bc046c4f38a9b973b38aaa22bf",
      "d230b807cc26494ab0baf3d5f3354f75",
      "db68f986bd7249799d4846ff0aaba7e8",
      "fcca9f2fd68e4cbebf49af4ba84cc99c",
      "6cfc307adb3f4fa7b4d15beb21425a90",
      "a295907501a7413bbe3253dfe4f78d03",
      "0b3db5c85fd74c3ba3433cdd12738939",
      "e072a927bc1c48d589bc62747d57f3bb",
      "df5eda8b7d604db4af952f0aff5fff60",
      "9e6c87ebe04b477e9fab3134a4638730",
      "a87571a7e6224d278080d984ef7fca4d",
      "47f2ab74c5a040728ccf7045edd816bf",
      "39458a0532804e8eb57d902e6c0b6b9f",
      "b1752b12f08146019a3277a596f89bad",
      "9de34089f22d428b9a85bb08b7ba4993",
      "dcaff64d53fc42f8bd5123c97ff4d0a8",
      "f4c90ea12f934b5484f5d30e49331199",
      "0b889f0ce5c4432195c25abaf159fc94",
      "94c7ac1ae56f4caa80fdcef928722bd0",
      "dc216af49911488c8ea4e7ecb34b4992",
      "7309809dbef34165a38018db662b7941",
      "bf994dba6daa406bb79736e16b403d8e",
      "4aa8a63f46e446f9838400f2178c230d",
      "f86463bc9f1547faa73aa5f2e148a274",
      "ac7851a039d047c1b18fb5f3631cde2b",
      "959d23f21fbd4c0c98d5bacd5c778a97",
      "b4a547d5ea294bbf857f6bed752abed7",
      "56ec09e5ec4049a1b8eccbcbd03ec0d2",
      "0c131e0766d44c8aa690cb936c646af2",
      "62a6d13595a74960ae92ef224c8a7952",
      "3e50e5920a4642de9994ca3151e4c526",
      "4c73176100bf47ad8e22486f5816d78f",
      "6ad1e7ed62e540f097098c08f1ce733b",
      "c17556b61e9349049f5f1fed4fd648c5",
      "05e1c183fb234ab1b7bba87346babd3a",
      "bcac50a75de04bae98943858f3437ae6",
      "133c20ae8e21481382102664efb12bc9",
      "ce7ae62f655747f0964925fe5c465d91",
      "4946f22f77014318a658f4eae1c01197",
      "b6120f2704bc4b00b57cb95cf547d19f",
      "5ea638e6c1b640d6b8b62ac7b5f69182",
      "7aa69372d7914b698ac312d93e2fd82b",
      "3869333b7e8e4f109d0bb8b165abdd83",
      "6002a2b682c14e1387d76b7bf3d8d38b",
      "c038303587c6413fa2b3db1219eb14b8",
      "afbf2d54675f465ea1b6dafaca7e734d",
      "28914bb112a64569938bef14dfcc2399",
      "65431a3b258740c097bbcfcb1df488d1",
      "79ee2dbc35ed4891ad1a4a5ea6975a0a",
      "75cf9ba91f924ccb9f9ab97fa4eb3f23",
      "529343403ed5446495bb9505ef9ccfde",
      "a9e92b1f9c4f4f158ad46adcd0a71193",
      "a3092f5b5bfd4955bc431532798635bd",
      "d097ec73a1754906a07ed34e4cab139f",
      "d2f54c27dfb04ce298a17e7362373fa7"
     ]
    },
    "id": "j0zKtLz_K7Uc",
    "outputId": "07111275-d5c6-4715-9639-9b578f3969d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449c4ec07cc24d1a9f037590accf2013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba9da72581f4538b9e91f097115c0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7ddfa0b36a4f27ad0ea37f27a71866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d20a8bc046c4f38a9b973b38aaa22bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f2ab74c5a040728ccf7045edd816bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa8a63f46e446f9838400f2178c230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17556b61e9349049f5f1fed4fd648c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c038303587c6413fa2b3db1219eb14b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configurar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Aplicar QLoRA\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TAeIyJ4pOZM",
    "outputId": "330bdf55-3164-4fdc-9a0c-d3a3cf510a30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A√±adir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "finetune_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "finetune_model.resize_token_embeddings(len(finetune_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "44d7e4011cd5496c8864bca6dc80b801",
      "b903a2ea756746098669a1708e243a4a",
      "5bd24c4d2d3e4139b5bbd33ceda41d37",
      "f2af455ff22446a1be0f093282b81c02",
      "2d05d08d6cdf4e79960eb5e1139754f9",
      "92054a87de974ffdbd21b2206326d555",
      "dddc7d153b7d45bdbbfb80929ac2e371",
      "3504d11966d94b21831e3c7de1d45ddb",
      "ed0b9e6b50034f7fab63f13fcdab2abe",
      "3ac670f738fe49c7b8c9c0e84c1feea8",
      "2a22e5bc34a841639cff8a6189eb4946",
      "3305c8008cd547f2a5fc892f9feb1b7d",
      "160f286843e348af962247a79507c22e",
      "b0852a00250e431991e0f9ec6bebb64b",
      "c7fdcb240a274a4ea7eb68ea1df167a7",
      "1a7ae74bfe374c05a97e1a85f147027a",
      "987002d8ee7244b4913ac9656b97b7bc",
      "2680e187c3cd4edcbb2a34a1925c32d7",
      "c7bc7a6994f24ae09802247dc8548b2f",
      "1cb01a2e91684cd09406f571d270eeda",
      "e632ccaf46cd4cf09bc787d3b9f8b3f9",
      "1bb9934a76e9417fb7c58332602dc748"
     ]
    },
    "id": "R2iPhpoQnSc2",
    "outputId": "76481526-39f5-497b-c2e7-271f6dd309ac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d7e4011cd5496c8864bca6dc80b801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3305c8008cd547f2a5fc892f9feb1b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = es_qum_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = es_qum_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "aXgDM4lAnagN",
    "outputId": "09e08acb-c4ae-473e-aced-ebbf1071c3c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-19-2171961071.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 04:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.136500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qlora_finetuned_qum_nllb/tokenizer_config.json',\n",
       " './qlora_finetuned_qum_nllb/special_tokens_map.json',\n",
       " './qlora_finetuned_qum_nllb/sentencepiece.bpe.model',\n",
       " './qlora_finetuned_qum_nllb/added_tokens.json',\n",
       " './qlora_finetuned_qum_nllb/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_finetuned_qum_nllb\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_qlora\",\n",
    "    save_total_limit=2,\n",
    "    bf16=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=finetune_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "finetune_model.save_pretrained(\"./qlora_finetuned_qum_nllb\")\n",
    "finetune_tokenizer.save_pretrained(\"./qlora_finetuned_qum_nllb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dazLUZlhKKHk"
   },
   "source": [
    "### 2. Modelo con Finetuning Ingl√©s-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "e2986057849c498f8f6654bee7c34214",
      "a169f9257b794cd9b494b95dd0f38d8e",
      "1f2baa738b4749dda309c71b6a813047",
      "753255a115f34c23991dcdd87cd4dd08",
      "4f4d0fb0bf2843d5b8729735da3ef9f0",
      "c5d2e0d9ffa44ec4af5478efcef22b12",
      "3eadaf0838264fca8e7b3545c2bd6e79",
      "46fc73721ace4f63a374b6e5a4d95648",
      "3127a852f1ef4d32bd8e27be19c11355",
      "83c9575a85e24621903afaf15012f205",
      "7bc50ca84517439f9c9c2f24d457d1e0",
      "3b5f552e890946f3b915e710ce0198e9",
      "946ef2d0c16e4cb8aaed25e3ea1db333",
      "4db67050037443c1afd03f9f1f03a5f1",
      "2c19ed51a38142f8a01d25619e7a6e4f",
      "c3382b5afdd74e2581150b9119f15b7f",
      "c9cd1f288ce04b3c94d7dc6d5cff24e4",
      "b3f48bd8141049679e938e5085a372b9",
      "89cf06d84f3b485db5d6d63f6832a149",
      "03b22ffba0a6455e862e371a9ec29978",
      "bcca652a491e4d1daae628a2641a3043",
      "1a4742fc417845dd96e9f0b948d5558b",
      "f3021cf9cd1246778ffce09ef73a0125",
      "3007df507d5845c38c413dea28fa036e",
      "1d2459bca3fb4e7990c857d03dfef871",
      "c578807c03de477b93cab5e2eaf23c0d",
      "5a5846f6f12448b5bef2a8c89e2e4bd9",
      "6fcaeddc80404dfd9f24b99fd4db5f86",
      "d702167de5b14f7795f2dd6cb8a71e0f",
      "a51cc680be354986a339d8483947686a",
      "fe8348ec0a1b4004a3c308e0a5e7bba2",
      "875609318c034f0db9c67f446fba01e3",
      "84154daf7cce478594bf4b29346c1913",
      "ffd7cbb0081d4db884851ae334e27848",
      "47d317d63111405497f3f253b3534b8b",
      "2dff662d6aa64bca92c533c3f5091609",
      "7c1e0f06cb444d46ba6069cbaa8f038c",
      "6c29b31862aa48b59c7d05a4c915dc62",
      "891adaf684ff41bbba513540d2a0733b",
      "ad3f9554e5d14a21bd5ab70fef795ae3",
      "8f73eb4d265b4e72b17db4b563d5f10c",
      "00faece2e95649b5b2d5f39b03abb5c0",
      "a5dd627485c14fd4aad2f937d54ca5f0",
      "430821b067c14e65b66569aea2f538b3",
      "5952329521244d718bff6fd1cc4d0a0d",
      "f1f0d4285dec429ca36963c47628965f",
      "bbb81fe8483f4eadba41900b2d046ac5",
      "6ec6555fc27c44bfb4db2c0c20b5b432",
      "b68b381b5301427ca86cf05070972cd4",
      "89e34892131541069c550537f0f21138",
      "087ed12f9de04e1191ef3c796e7c0e4a",
      "f2a150f69c30465f8f0464b696f729a4",
      "f6e2f896536648d8b23e97ffbd05fdef",
      "913b957c01654f8eb18e194d9587f4ae",
      "d13662f64f1445389b02a2d40c01e7dc",
      "68c4791de02e4442a6d3c2aeadb64634",
      "9611ddebfb4248b79d9843ec82fe9115",
      "96302ac40c5e408eacb6ee72ef4757d4",
      "72b8df47e4014f64b071fb115e22d80c",
      "988ffe33ea59421783c0356cb5edcb68",
      "fa3ccd5ffb704d0c97728e07f868fb6c",
      "f72b3d55a45c4c7d8a3d04a75dd4b11a",
      "c6fdd8658baf4d59b241631d2fc4f875",
      "60dbcb184d604968a1830aa3a7c7c8e2",
      "a0ce619326ac4915acabd88b8bc85cb8",
      "45214671e9ac4a76af8a18dd2e38015a",
      "a41b4ec251e24d55bd89b0531ebeffe5",
      "c1ca37e604a14abf97c5ce8df21424b1",
      "895226062800456eb30a8dcc20215ba2",
      "8b18a1629dc34d679e09050945fe46b5",
      "784ca8bedfab49379cc394318d2f0b53",
      "94743ea58b0d4baa97fbbba4c55d1d7f",
      "254d300a8d084202883da64a3a400e38",
      "c4abb342fdc84c01bd273e403ee9e8b5",
      "caa654cb425c4a9fb5dc19a30f76d58b",
      "971f7a4aeaf548159d2810173fb60af3",
      "49cbdc9fedfd4d1d9c97c1d6e9eebf7a",
      "28a1733700654a53a30c3791d213783b",
      "2c018b8f5bfc46ad9768a0ba67474f6e",
      "e041e5815d724d949b09b617034b831f",
      "14184416bab84361818c2c40c67d8c55",
      "cb9fd0d9fa2c4e959a063c4f6a2b1d56",
      "c4dbef832863421a95c2a4ac84639422",
      "5fc3650942ba4c5da75e55e4701febcf",
      "e6255d126d5a409db260a10a9d79d8e4",
      "e22bbb8c7cd9420a9474039a935cbb65",
      "b6c9387655b04d6a9913944cb1b6e872",
      "b86c5912707e44728291ea4bfa44f5e3"
     ]
    },
    "id": "LEUkIjpj33ln",
    "outputId": "b0e5a1c2-242e-422c-daa7-ce7943a4bfe2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2986057849c498f8f6654bee7c34214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5f552e890946f3b915e710ce0198e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3021cf9cd1246778ffce09ef73a0125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd7cbb0081d4db884851ae334e27848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5952329521244d718bff6fd1cc4d0a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c4791de02e4442a6d3c2aeadb64634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41b4ec251e24d55bd89b0531ebeffe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a1733700654a53a30c3791d213783b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "en_qum_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "en_qum_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stma1J90dTmA"
   },
   "outputs": [],
   "source": [
    "# A√±adir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "en_qum_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "en_qum_model.resize_token_embeddings(len(en_qum_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "cc1dee25a6ea42f293b3c1452e614b45",
      "5ae282dcaebc4532b07911cfde8c476a",
      "bda7a75b2578438b9804f06ec48f9ce1",
      "71d3d1a788f248e9b305c457768eb964",
      "36c9a6a812bc43969da1fa528673813e",
      "6e3432c7f9994e83a4d756f79e8d1f41",
      "b80944bd8ef646e691336ceb94efa0b1",
      "39b685495cad461996c0406b43d5dfe7",
      "fe42c04a5857405d9a425a8a58262ca0",
      "8dee820a5ff946f7a09e95ab54082506",
      "52560adc15234804b1fa33a4cddb7013",
      "5d6a989635ee4ee6ae64f9a7c9457f3d",
      "533ab382d36546ceb9dd9d1c5668dffe",
      "0939b8080c9d4a058063d689fd03622e",
      "047b83d3b15445c49658d75ebade4aa0",
      "5dc440ae793145a2848af65d7bb0b600",
      "e75e444d3e3c4b0eb7427a1c992c0e85",
      "db7eb15a5d924f75924e10bc70ed0276",
      "58d70ef0f2c34635b9495a1fcc42eb8e",
      "b5ab57e926f9486f8fdcde87b2152bb8",
      "d762ea66d4804d7aba2e5640f219ccbb",
      "07f8894fa83c4d4999a9530194cad4a3"
     ]
    },
    "id": "w1sEWghP4B7C",
    "outputId": "c80ab1e6-da08-4ca7-971b-38c92103ed1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1dee25a6ea42f293b3c1452e614b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6a989635ee4ee6ae64f9a7c9457f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function_en_qum(examples):\n",
    "    max_length = 128\n",
    "    inputs = en_qum_tokenizer(examples[\"input\"], max_length=max_length, truncation=True, padding=True)\n",
    "    targets = en_qum_tokenizer(examples[\"target\"], max_length=max_length, truncation=True, padding=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_en_qum_train = en_qum_dataset[\"train\"].map(preprocess_function_en_qum, batched=True)\n",
    "tokenized_en_qum_test = en_qum_dataset[\"test\"].map(preprocess_function_en_qum, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxivqDvt4GEf",
    "outputId": "a9c3425e-49e3-4be1-df1e-0531be05ba4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args_en_qum = TrainingArguments(\n",
    "    output_dir=\"./finetuned_en_qum_nllb\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_en_qum\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "WWYUxQWm4IU_",
    "outputId": "37c74abf-e1ee-419b-df05-bda9e7639303"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-15-1310942191.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_en_qum = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>8.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>8.157500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=8.834854166666666, metrics={'train_runtime': 93.4692, 'train_samples_per_second': 32.096, 'train_steps_per_second': 4.012, 'total_flos': 815382134784000.0, 'train_loss': 8.834854166666666, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_en_qum = Trainer(\n",
    "    model=en_qum_model,\n",
    "    args=training_args_en_qum,\n",
    "    train_dataset=tokenized_en_qum_train,\n",
    "    eval_dataset=tokenized_en_qum_test,\n",
    "    tokenizer=en_qum_tokenizer,\n",
    ")\n",
    "\n",
    "trainer_en_qum.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc2QH1HJcuKn"
   },
   "source": [
    "## Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9jCIC05CCG_"
   },
   "source": [
    "### M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "f9d247c2f65340c5bf2a5cd35c321e64",
      "b03931a6c6024a65ba2ef20dc5c65851",
      "d0e0f9139ecf4bc6a77b6d82cc6dc424",
      "9dd0b22c831247528fc8b486033d3cf6",
      "ecf4cd8b1cf44a7f91e756a9a466531e",
      "147450e388a34765b5e7b731fe0413bc",
      "73528c48552b4b6e8475c3ec592659b5",
      "ce9cd1568c8d425eb642338b26871edb",
      "e43f48c489244132bee8dd11e44c6aaa",
      "cd78859a5f194707b9215a5e22baaa03",
      "21b21488df804b07b98b3321304fe14a",
      "bfb14264114f4650aeebec5bf0b8c013",
      "11d93098b86c4e75bc85cfc1812a2196",
      "0a3c88456431484d865353f20b5cccea",
      "599b78a278a04abd959e5ecc9b19a549",
      "e3c3dc8045ce4482ab6b03732ed04d3d",
      "99563cce79ac408a8600fe859ec7d1d4",
      "01ec2c49d16d48f38d275637b947f660",
      "abf8a4c0bcd840799aacb031b7857d88",
      "e64f7b5731b84b00b82b9c18b52b187b",
      "d06f4759cb1a499b89eea46aeb7ef0c7",
      "92eaa870bc844e919db29aa5bbf84caa",
      "34e1d312443a4322ae77898fad03c609",
      "4ef975b1366445189fd3470d2daf6dd6",
      "77a7edc7653e49be906d055de87e94e2",
      "376ef97aa7b442e6aeaf9b55f4f0994e",
      "f76cb7e4c5a5445ba267815cb8e17e8f",
      "20240da8d4494a8291ac699f316b8252",
      "1bf53a30f92b47af81994849ce790958",
      "dd253bf905fb457ea825a031b42b3e13",
      "2f7886bc6fa647b68cab36de54ad1e8f",
      "92700430ca0c409ab152cc521e471dc1",
      "cf71708db79d4e51a0bed30b0aabb6ae",
      "f5ad4528fba441f3aea85ef4e37bf5de",
      "8677026b661c45298459f7493aedb2a1",
      "bce1ecdd3b394ea9933582bcd7b4163c",
      "12f5741cffe44621b3c86e9b3ecd1310",
      "36e4e4cfee814d52b96df1b546984773",
      "b3f41f3591fb4db595bd8f4c34a5e723",
      "26232c8e58894ffea2069c60f0cc03c6",
      "05d3710ff19b45af878821b6c473956d",
      "1203c348f1ea4738bcea8d1e3afb399e",
      "02a6cdfb4dc141b4b824dc03f3f485c1",
      "017005011b634d2abf3de1bc31e5c50f",
      "1111f5a5d664480fbb23c7702400bcbe",
      "7f658591e8c647e296b3d2e52570f30d",
      "d6c243ddbaa84363b4aa7420c80c726c",
      "08ec4e2fa3704624a91e609500d00192",
      "517730775c054a838eff14e47c01ca49",
      "aeee154ac5dd4eccad1b2a90205df689",
      "d5010cbb08114f07aa5dd54747195223",
      "33a281bbf2724fea8454455dc9bcea1f",
      "73fe869722b6450a872c8feb1dfee232",
      "7df87968c3eb4aa58b453a40b4978831",
      "aaf917c6d37f41cf90e4ecdbd95a61be"
     ]
    },
    "id": "YEoaaYAZCDnX",
    "outputId": "934a4ee5-f01b-4675-9f3f-8484eeb095bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d247c2f65340c5bf2a5cd35c321e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb14264114f4650aeebec5bf0b8c013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e1d312443a4322ae77898fad03c609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ad4528fba441f3aea85ef4e37bf5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1111f5a5d664480fbb23c7702400bcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coKh9lECcw45"
   },
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "m7IRRMdMiMtN"
   },
   "outputs": [],
   "source": [
    "origin_lang = \"eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjB3SBkqn1JG"
   },
   "source": [
    "#### 1. Con Finetuning Espa√±ol-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uBCVbR3obZG",
    "outputId": "0b7b3a7b-8507-4b62-9d54-d9a5b12741db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Traduciendo: eng ‚Üí Espa√±ol ‚Üí Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321/321 [09:49<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_intermediate = []\n",
    "references_intermediate = []\n",
    "\n",
    "print(f\"\\nüîπ Traduciendo: {origin_lang} ‚Üí Espa√±ol ‚Üí Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    # Paso 1: origin_lang ‚Üí espa√±ol\n",
    "    inter = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"spa_Latn\")\n",
    "\n",
    "    # Paso 2: espa√±ol ‚Üí \"Sipakapense\"\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, inter, \"spa_Latn\", \"qum\")\n",
    "\n",
    "    predictions_intermediate.append(pred)\n",
    "    references_intermediate.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qVv0v6pUvm-e"
   },
   "outputs": [],
   "source": [
    "clean_pred_intermediate = [pred.removeprefix(\"qum\").strip() for pred in predictions_intermediate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2roPLoJoy2L",
    "outputId": "dba8faa1-72bc-48af-8db4-a123caef91e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' wife only uses good wood.\n",
      "> Referencia:  wu rxqiil ta º xuux xaq utz laj si º kchuknik chre\n",
      "> Predicci√≥n:  ri xtz ºi º rb ºal\n",
      "\n",
      "> Entrada:     #qum# All the peaches are\n",
      "> Referencia:  Njeel ri xkarawa ºn xpoch ºik rech chenim xchaq ºjik\n",
      "> Predicci√≥n:  k ºo ºb ºal\n",
      "\n",
      "> Entrada:     #qum# Our people don't speak our language anymore.\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolb ºaal\n",
      "> Predicci√≥n:  ri ri x ºo ºb ºal ri xb ºa º\n",
      "\n",
      "> Entrada:     #qum# The student is learning how\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri su º\n",
      "> Predicci√≥n:  ri xtz ºi ºb ºal xk ºo º\n",
      "\n",
      "> Entrada:     #qum# The woman was hugged by her husband\n",
      "> Referencia:  Ri ixoq tz ºulmaj rum ri rchjiil\n",
      "> Predicci√≥n:  ri xk ºal xb ºal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicci√≥n:  {clean_pred_intermediate[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcM-8kilozFz",
    "outputId": "fd2eb484-309a-4278-aeeb-0cc4a0949f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√©tricas BLEU ===\n",
      "BLEU-1: 14.31\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== M√©tricas ROUGE ===\n",
      "ROUGE-1:   12.78\n",
      "ROUGE-2:   0.33\n",
      "ROUGE-L:   12.37\n",
      "ROUGE-Lsum:12.35\n",
      "\n",
      "=== Otras M√©tricas ===\n",
      "METEOR:     5.94\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_intermediate, references=[[ref] for ref in references_intermediate])\n",
    "\n",
    "print(\"\\n=== M√©tricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== M√©tricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Otras M√©tricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MTsr6mJBWrm",
    "outputId": "862ba8ab-345a-45f6-c501-fed461b4dee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\n"
     ]
    }
   ],
   "source": [
    "with open(f\"results_{lang}_qum_finetune_intermediate.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_intermediate, references_intermediate):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(\"‚úÖ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_UFY-dToFsb"
   },
   "source": [
    "#### 2. Con Finetuning Ingl√©s-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0UrnNGqobuo",
    "outputId": "017e9614-44bb-42de-89fa-cc45a4298593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Traduciendo: eng ‚Üí Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321/321 [03:59<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_direct = []\n",
    "references_direct = []\n",
    "\n",
    "print(f\"\\nüîπ Traduciendo: {origin_lang} ‚Üí Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"qum\")\n",
    "\n",
    "    predictions_direct.append(pred)\n",
    "    references_direct.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "chF42cMCu0k2"
   },
   "outputs": [],
   "source": [
    "clean_pred_direct = [pred.removeprefix(\"qum\").strip() for pred in predictions_direct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGGj_XKctmn9",
    "outputId": "57e30dd5-7548-467e-fe07-e560767a3449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' wife only uses good wood.\n",
      "> Referencia:  wu rxqiil ta º xuux xaq utz laj si º kchuknik chre\n",
      "> Predicci√≥n:  q ºi º ri xb ºal xb ºal\n",
      "\n",
      "> Entrada:     #qum# All the peaches are\n",
      "> Referencia:  Njeel ri xkarawa ºn xpoch ºik rech chenim xchaq ºjik\n",
      "> Predicci√≥n:  ri xb ºal xb ºaxb ºal\n",
      "\n",
      "> Entrada:     #qum# Our people don't speak our language anymore.\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolb ºaal\n",
      "> Predicci√≥n:  r ºi º rxb ºi º\n",
      "\n",
      "> Entrada:     #qum# The student is learning how\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri su º\n",
      "> Predicci√≥n:  ri xk ºo º\n",
      "\n",
      "> Entrada:     #qum# The woman was hugged by her husband\n",
      "> Referencia:  Ri ixoq tz ºulmaj rum ri rchjiil\n",
      "> Predicci√≥n:  ri xtz ºal xb ºanb ºal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicci√≥n:  {clean_pred_direct[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNuiAUOytvUu",
    "outputId": "99963e70-02ad-4b7c-fbee-515106a83481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√©tricas BLEU ===\n",
      "BLEU-1: 12.58\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== M√©tricas ROUGE ===\n",
      "ROUGE-1:   9.73\n",
      "ROUGE-2:   0.04\n",
      "ROUGE-L:   9.35\n",
      "ROUGE-Lsum:9.36\n",
      "\n",
      "=== Otras M√©tricas ===\n",
      "METEOR:     4.03\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_direct, references=[[ref] for ref in references_direct])\n",
    "\n",
    "print(\"\\n=== M√©tricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== M√©tricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Otras M√©tricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2o0_ZKITBrRf",
    "outputId": "04c6592d-8989-49c8-fa1f-2bf9e59e6926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados guardados en 'results_en_qum_finetune_direct.tsv'\n"
     ]
    }
   ],
   "source": [
    "filename = f\"results_{lang}_qum_finetune_direct.tsv\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_direct, references_direct):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(f\"‚úÖ Resultados guardados en '{filename}'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g5rb6MT0A5mD",
    "R6AMhN2GZpNt",
    "Q5ukIdzqYhCo",
    "o6ngW-l5c1aq",
    "qjWCp3bkc27u",
    "dazLUZlhKKHk",
    "tc2QH1HJcuKn",
    "U9jCIC05CCG_",
    "coKh9lECcw45",
    "ljNvMeB3ezyA",
    "rjB3SBkqn1JG",
    "y_UFY-dToFsb",
    "fCaQTajaoPyr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
