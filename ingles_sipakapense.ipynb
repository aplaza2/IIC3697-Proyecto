{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96PapqzwAUkB"
   },
   "source": [
    "# Experimentos de Traducci√≥n Ingl√©s -> Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_FHD9kw266t",
    "outputId": "69a8c951-cbb9-443c-b074-99720455b3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yheiPHx8YN0m"
   },
   "outputs": [],
   "source": [
    "lang = \"en\"\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5rb6MT0A5mD"
   },
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yFaCwcFLx9HV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hg_qHsB95EzQ"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NF14LcemARSd"
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate peft transformers sentencepiece datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7-HRnAKmA8LR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8Nds_8Y1Mm"
   },
   "source": [
    "## Datasets\n",
    "!REQUIERE SUBIR LOS ARCHIVOS DEL DATASET SINTETICO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smVHNrAeY5ck"
   },
   "source": [
    "Dataset se puede encontrar en el [siguiente enlace](https://github.com/transducens/mayanv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1ViYHauY4v3",
    "outputId": "48f3c925-03a2-4c0e-c79a-4754d18d2ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mayanv'...\n",
      "remote: Enumerating objects: 177, done.\u001b[K\n",
      "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
      "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
      "remote: Total 177 (delta 22), reused 157 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (177/177), 1.35 MiB | 3.79 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/transducens/mayanv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ghoF4-noMjaC"
   },
   "outputs": [],
   "source": [
    "def load_lines(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def generate_dataset(language, train_folder=\"train\", test_folder=\"test\", base_path=\"mayanv/MayanV\"):\n",
    "    # Rutas\n",
    "    train_lang_path = f\"{base_path}/{language}/{train_folder}/data.{language}\"\n",
    "    test_lang_path = f\"{base_path}/{language}/{test_folder}/data.{language}\"\n",
    "\n",
    "    train_es_path = f\"{base_path}/{language}/{train_folder}/data.es\"\n",
    "    test_es_path = f\"{base_path}/{language}/{test_folder}/data.es\"\n",
    "\n",
    "    train_src = load_lines(train_lang_path)\n",
    "    train_tgt = load_lines(train_es_path)\n",
    "\n",
    "    test_src = load_lines(test_lang_path)\n",
    "    test_tgt = load_lines(test_es_path)\n",
    "\n",
    "    # Crea datasets\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_src, \"target\": train_tgt})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_src, \"target\": test_tgt})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PmYZbWQ1kPpQ"
   },
   "outputs": [],
   "source": [
    "def invertir_input_target(example):\n",
    "    return {\n",
    "        \"input\": example[\"target\"],\n",
    "        \"target\": example[\"input\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "80463f537d6a4a06a048707862bc3757",
      "b6627efd17ec4af280c234fa40de518e",
      "7a92681906ca4217a6c6fe765398f703",
      "103cced007154ac9890c1623463c0744",
      "3d93d8443fb1431db1beacc06dd423e2",
      "a457124b1ac741f7b7583febe7401165",
      "96e3917db4264d0ab99043276e7b0ca1",
      "13bb02e875de4d2cbfac13700ecfbf05",
      "8157b2e1038543fc826a5b1f93be7975",
      "5251f1499cfd42a3aab538eef9a2caa0",
      "78f10c752dc3493c8770992d8814b54d",
      "7e51348a745344329496d86102db0f4a",
      "4a75012ce7754e89be28533ecbd030ae",
      "a16868c0f44849908d214e7173cc557a",
      "16fe00b3d29643b0b45c14ce2dbe6fc9",
      "54f62b5d853145e68f340e58f029d6b6",
      "9ac1818be10b4b539745a18759318289",
      "86fc7f13bae04d71bd03f42310795629",
      "01724037e4764e608880ed97a457738d",
      "f936cad5b43b40c493e0e1b76d2b0bb3",
      "6111bbe1a281402b9652655933964641",
      "4d7138e980c14f7983a7280089e4692d"
     ]
    },
    "id": "4l0xcZcOgsmx",
    "outputId": "ecb06aa3-67ad-46e2-ecba-e5b6c83a6082"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80463f537d6a4a06a048707862bc3757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e51348a745344329496d86102db0f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_qum_dataset = generate_dataset(\"qum\", train_folder=\"test\", test_folder=\"dev\")\n",
    "es_qum_dataset = DatasetDict({\n",
    "    \"train\": es_qum_dataset[\"train\"].map(invertir_input_target),\n",
    "    \"test\": es_qum_dataset[\"test\"].map(invertir_input_target),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e-p1Rq94bTJL"
   },
   "outputs": [],
   "source": [
    "def generate_en_qum_dataset_from_files(\n",
    "    data_dir=\"./\",\n",
    "    base_path=\"mayanv/MayanV\",\n",
    "    language=\"qum\",\n",
    "    train_folder=\"test\",\n",
    "    test_folder=\"dev\",\n",
    "    lang=\"en\"\n",
    "):\n",
    "    # Paths a los archivos .en\n",
    "    train_en_path = os.path.join(data_dir, f\"train.{lang}\")\n",
    "    test_en_path = os.path.join(data_dir, f\"test.{lang}\")\n",
    "\n",
    "    # Paths a los archivos .qum\n",
    "    train_qum_path = os.path.join(base_path, language, train_folder, \"data.qum\")\n",
    "    test_qum_path = os.path.join(base_path, language, test_folder, \"data.qum\")\n",
    "\n",
    "    # Carga las l√≠neas\n",
    "    train_en = load_lines(train_en_path)\n",
    "    test_en = load_lines(test_en_path)\n",
    "\n",
    "    train_qum = load_lines(train_qum_path)\n",
    "    test_qum = load_lines(test_qum_path)\n",
    "\n",
    "    # Validaci√≥n\n",
    "    assert len(train_en) == len(train_qum), f\"Train mismatch: {len(train_en)} vs {len(train_qum)}\"\n",
    "    assert len(test_en) == len(test_qum), f\"Test mismatch: {len(test_en)} vs {len(test_qum)}\"\n",
    "\n",
    "    # Combina en datasets Hugging Face\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_en, \"target\": train_qum})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_en, \"target\": test_qum})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })\n",
    "\n",
    "\n",
    "en_qum_dataset = generate_en_qum_dataset_from_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6AMhN2GZpNt"
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4fo2s-t9Zozn"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 128\n",
    "\n",
    "    inputs = finetune_tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    targets = finetune_tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def translate_text(model, tokenizer, text, src_lang, tgt_lang):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    forced_bos_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "    # Detecta el dispositivo (GPU o CPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokeniza y mueve a la misma device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Generaci√≥n\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def build_and_save_split_en_only(split, model, tokenizer, output_dir):\n",
    "    input_path = os.path.join(output_dir, f\"{split}.{lang}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"w\", encoding=\"utf-8\") as f_en:\n",
    "        print(f\"üîÑ Traduciendo y guardando split '{split}' al {lang}...\\n\")\n",
    "\n",
    "        for example in tqdm(es_qum_dataset[split]):\n",
    "            es_text = example[\"input\"].replace(\"#qum#\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                en_text = translate_text(model, tokenizer, es_text, \"spa_Latn\", f\"{lang}_Latn\")\n",
    "                en_text = \"#qum# \" + en_text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error traduciendo: {es_text}\\n{e}\")\n",
    "                en_text = \"#qum#\"\n",
    "\n",
    "            f_en.write(en_text + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Archivo '{split}.{lang}' guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5ukIdzqYhCo"
   },
   "source": [
    "## Crear dataset sint√©tico\n",
    "!SOLO EJECUTAR SI NO SE TIENE EL DATASET TODAVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOjTDQjmYq8F"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "mid_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "mid_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "mid_tokenizer.src_lang = \"spa_Latn\"\n",
    "mid_forced_bos_id = mid_tokenizer.convert_tokens_to_ids(f\"{lang}_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U4gFjfFYots"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/en_qum_dataset\"\n",
    "\n",
    "build_and_save_split_en_only(\"test\", mid_model, mid_tokenizer, output_dir)\n",
    "build_and_save_split_en_only(\"train\", mid_model, mid_tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6y8Xh2RBAmS"
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWCp3bkc27u"
   },
   "source": [
    "### 1. Modelo con Finetuning Espa√±ol-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "ed877ff6c771498b9c4d4928b5d7370f",
      "0f2872267f6b4bb3949f5fe2c83a5124",
      "5d558e203e3845d6974272bf1d9e1444",
      "f278417c2a40449998da047a0e25dbed",
      "73e124a326f447209651635e370db6b3",
      "22263ede295b4401ba04c1174a0133ee",
      "3bf165cda1d64c36abf2e225021e90b8",
      "b314d5d9cf274abfb3b7c68571f3f569",
      "3878b48b639c455787a187417e3b4cd8",
      "ed09feeadd97448db1dbb332322d5f6b",
      "9629936fe3024a80b6ea88251d7482f3",
      "c822b4e131504977b14280b4317af59e",
      "2a7ca5fa265546d4a3e61f24126e8461",
      "9a8e90f9180a4b34895129b17eb651b4",
      "f3395bda08894134900e5d232d1e796c",
      "ca5b303c4dd34ca7836a1490495d6df2",
      "8dca799b59334d2b9e8485bccc7da715",
      "50f14c5c36574c2d802e3cb6dbb4bc38",
      "30ff4c2276fb40c28d2bac7b9425e6c3",
      "d79b459306034e4d84355fe6aeaae44e",
      "9a4e541a96cb42d2a31caf12d5762eb1",
      "e3f0e788deb04d96aeab571bb35c7a1c",
      "a5e85a0c308a4e629d968abeb5482284",
      "d7f6eecbece1450bb2b076690a14595e",
      "52108039dcb94d4893360a9ba04ed0a3",
      "f57e24108b81411f858266da992d0632",
      "5bc09a9160b14226a669e557482cd70d",
      "6128c3609d474933994a347112de1dd1",
      "55ae7d858b8d42c7b9675da71c1a1026",
      "acb938dad17e4f1fb8b368e433b59c43",
      "5e14838370424410adcda3e49c57a9b6",
      "586b1f710bc44b798b41ec6dc96caf04",
      "97139153faf9482e9617bcc013336d00",
      "c7c25faaf26746fe8a66c6d1b490b157",
      "d927be8f2a60486face0a307fb17bb2d",
      "31d39da77ca14e5390de0416ed712e97",
      "ecf339cd128d423a802622bee1301f06",
      "2378af72dd1148fe984d39a626c6ac8a",
      "a46a30b47ad74b258d7ff6e68fc29058",
      "5033ddfd501c4b5bb526f3e75b1f374a",
      "ce68cfbbe4224a03a468b022c62d5954",
      "502930d3cd494d3a90cd2d31e2b68028",
      "4c7c5d9ac2804287a0476d4b522926ac",
      "084f9f26c10f4acba77788edf4f7404a",
      "54b1d71a81c0436295230f9051e85c7c",
      "2437dfd7ae7d4ecd840d0db5ee11dcb7",
      "aa590872a7d24cc2a68960f66f1327c4",
      "4b52510a312045588f4dc7f45b5e43c7",
      "190d57b605c74448a76631b3d413e9df",
      "e768371d3eef4a9894ddb3a71fd6f1c9",
      "f4d4c5fc705847f7ae1efbcf565cf4c2",
      "1ca41738ec10498d834f5470f7ea16ca",
      "0e71eb92a89d4e99ae749bcf527f3d5d",
      "57b23e6747f649d994cdb32dec6ccbed",
      "b406a9537b6a46a3a7106c1cf92ca411",
      "c7d0ab4d5f5046cb999628b3482087dc",
      "07daae72b93040c6b0cb249468930f73",
      "ad1878f822ae4cbcb67f8c8c63873c2c",
      "34cb0278e3234d75b84e1818a4ad9f93",
      "dc3275aa4a864aec8bcaf42b4e9492bb",
      "f10a21c3193f42c981c8b64de8d1f4a3",
      "78520f9883924a588563c307fd42c085",
      "4aaa0644f0f94f7ca4d0f751150e861c",
      "a6c803f557844e92a164ce3130878945",
      "be4d73ce2b3740009eb22213b75b58ae",
      "d943bd39296141179447a4744f009748",
      "e1cb64460a5a4e0880e4755d3a2bb229",
      "d7b676b42d104c6cb9c164bcb6e6924b",
      "781b026636db43018125a51003550066",
      "55de7f8426e84234981e88ddf01e6456",
      "a6e1671709fd4516ac1117f2b6335e33",
      "e702e6611ef6497488e61bf231481c44",
      "e3100a3310004fc981462191c3da9550",
      "893a90ad46b3424fae28c2c8106f3dc7",
      "bb641e5cf86249b984cbea3b41c71e0b",
      "00b595a6346f420a8865f53db5fad71f",
      "3766d623849d4dfdbe4405ae6f9700c9",
      "f967ec86b83448399f49fefc26a77d85",
      "2204a25e409d43919c12bc4ab9e0f93f",
      "626fd8566d99485ab97a5547c5ad7bc4",
      "19238a7e399d4e50a00173a7b26fccb9",
      "8db726fd36e849cabc659a636189b85e",
      "b3aa4cc6fc1a4f719fb04b19f0a3b8bf",
      "cffac417c3224cacad0e6bb015db89cf",
      "48e38832e94e4c098b53e1711b169ed5",
      "df8f4dba0aea4a0f89eb12acba89702e",
      "d3f7338915d5469898e784b7aed2da77",
      "fb552e778bd64226b84f43b63c2c9fb9"
     ]
    },
    "id": "j0zKtLz_K7Uc",
    "outputId": "607e8d96-c6b1-4ce6-b6cf-d99462fa91b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed877ff6c771498b9c4d4928b5d7370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c822b4e131504977b14280b4317af59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e85a0c308a4e629d968abeb5482284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c25faaf26746fe8a66c6d1b490b157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b1d71a81c0436295230f9051e85c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d0ab4d5f5046cb999628b3482087dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cb64460a5a4e0880e4755d3a2bb229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f967ec86b83448399f49fefc26a77d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configurar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Aplicar QLoRA\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-TAeIyJ4pOZM",
    "outputId": "7893a043-5469-46f4-de52-337a3a857483"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A√±adir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "finetune_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "finetune_model.resize_token_embeddings(len(finetune_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "85d777596e0341f39702b87d649840cd",
      "4e70477523c143cb820b4d06e3d99674",
      "5486617a07b64d688b3851f2ff677e68",
      "dba23a24ab0b4a1a84cca5fbf5b31bdc",
      "a6761e18db404120adc3645c6bb5ed1a",
      "86960611c9ff4a5ab9edaa5995a7f9a9",
      "40e08433524b44adbf0e63a4c1c4ef86",
      "b0894800f292473998691518377e3482",
      "c6f0f7a23d5d46cfba7eb22b7d0026eb",
      "757b5401e41f41348fd90f23b7e5d5f9",
      "4fa6595d6d2b4c3a8850cd9dd3d3ba6f",
      "a9c0fb5caed940d29e3921327b891689",
      "bb151affdaf4475983a456e963354811",
      "e4299e189a4b44d185b1a277f56f41b7",
      "a98d657cd1a14712bf9ac83e90586887",
      "c75b8622b2194685a920682c3aed9cbf",
      "186fc1076cf142e381d4d9375bda2a46",
      "2a6d63af6f83489a9523d7b01b02e971",
      "cb95f4c7c8cb40f786a1ad920ace432c",
      "a7ec6bc278cd4e998a9ccf30fa9fbda6",
      "04c042fb5d9a4db2a779da5d94ed94d9",
      "2a7ea1a7411a454b85bcf6967194bb0c"
     ]
    },
    "id": "R2iPhpoQnSc2",
    "outputId": "7f317c9e-5dbd-4f71-95c7-eb179b9dcc8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d777596e0341f39702b87d649840cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c0fb5caed940d29e3921327b891689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = es_qum_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = es_qum_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aXgDM4lAnagN",
    "outputId": "593c3d88-467f-4875-f199-d8dbd699e570"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-15-1857230931.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 19:55, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>7.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>7.028200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qlora_finetuned_qum_nllb/tokenizer_config.json',\n",
       " './qlora_finetuned_qum_nllb/special_tokens_map.json',\n",
       " './qlora_finetuned_qum_nllb/sentencepiece.bpe.model',\n",
       " './qlora_finetuned_qum_nllb/added_tokens.json',\n",
       " './qlora_finetuned_qum_nllb/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_finetuned_qum_nllb\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_qlora\",\n",
    "    save_total_limit=2,\n",
    "    bf16=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=finetune_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "finetune_model.save_pretrained(\"./qlora_finetuned_qum_nllb\")\n",
    "finetune_tokenizer.save_pretrained(\"./qlora_finetuned_qum_nllb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dazLUZlhKKHk"
   },
   "source": [
    "### 2. Modelo con Finetuning Ingl√©s-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LEUkIjpj33ln"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "en_qum_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "en_qum_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Stma1J90dTmA",
    "outputId": "5b52599e-8e73-4871-d177-6926ee30d755"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A√±adir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "en_qum_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "en_qum_model.resize_token_embeddings(len(en_qum_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "ee43615cffd14804980d4b340e98db60",
      "bb82583d438d459db6347665bf0ac8dd",
      "20f7bd3b474c4a6a81a3e9249e665b24",
      "bbdae53d922c4af9aa532666024c5387",
      "27f43b4642394d80aaebbed549c3c01c",
      "dba087e13103408d94eb1739711d317b",
      "222d377e6b9f4a1298cf77fd6361ee76",
      "990de8c6945f448c92c97ce7b33fe745",
      "b8dd4175e56b468fbe6425843ca1ab1d",
      "605629a82a1e452faabb965280b560cc",
      "604f3f980aa941ba86ce0b40c9068efb",
      "d26d7f64c30441838715e0e24a0b11ee",
      "0e07f4ae534a43318e03780fb073348a",
      "6879968329554dca8e1660078aaa8cef",
      "f835c3ec7d9b429395638a3cf433b638",
      "e9658fa4e19d48919a4b67255ff49476",
      "2185d0e4f2804dbb97857da7c1c69418",
      "46504dde9f9c45eaaabb8a29b5c480f2",
      "e37b327e8956446bb40d550d7857912d",
      "c771ca1f84e647818d7125ba42f0848c",
      "2c935798f9044e63ab806ce298d9611e",
      "96a41dcc9dd3489bbdea057b9c48828b"
     ]
    },
    "id": "w1sEWghP4B7C",
    "outputId": "e851132d-6529-44ef-eb70-de6bdc159e58"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee43615cffd14804980d4b340e98db60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26d7f64c30441838715e0e24a0b11ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function_en_qum(examples):\n",
    "    max_length = 128\n",
    "    inputs = en_qum_tokenizer(examples[\"input\"], max_length=max_length, truncation=True, padding=True)\n",
    "    targets = en_qum_tokenizer(examples[\"target\"], max_length=max_length, truncation=True, padding=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_en_qum_train = en_qum_dataset[\"train\"].map(preprocess_function_en_qum, batched=True)\n",
    "tokenized_en_qum_test = en_qum_dataset[\"test\"].map(preprocess_function_en_qum, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxivqDvt4GEf",
    "outputId": "244c1639-0343-4f9a-f0f4-101e374a64b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args_en_qum = TrainingArguments(\n",
    "    output_dir=\"./finetuned_en_qum_nllb\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=n_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_en_qum\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WWYUxQWm4IU_",
    "outputId": "986e29e5-539b-431e-8590-61dd0cbb0c83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-27-1310942191.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_en_qum = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 21:00, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>8.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.638200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>6.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>6.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>6.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>6.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>6.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>6.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>6.222500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=6.957568001302083, metrics={'train_runtime': 1260.9977, 'train_samples_per_second': 11.895, 'train_steps_per_second': 1.487, 'total_flos': 764420751360000.0, 'train_loss': 6.957568001302083, 'epoch': 15.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_en_qum = Trainer(\n",
    "    model=en_qum_model,\n",
    "    args=training_args_en_qum,\n",
    "    train_dataset=tokenized_en_qum_train,\n",
    "    eval_dataset=tokenized_en_qum_test,\n",
    "    tokenizer=en_qum_tokenizer,\n",
    ")\n",
    "\n",
    "trainer_en_qum.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc2QH1HJcuKn"
   },
   "source": [
    "## Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9jCIC05CCG_"
   },
   "source": [
    "### M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "f609c2a5fd8d4811b9a1b1eea5208ac7",
      "809ab54ec9df4fff83debd5e3e3e4eb8",
      "1f50e2636dc040dcaa69bc86d9bc4586",
      "e2ff98d3f5f7481fa7e05fcbaba7bd64",
      "2ffdca8e8b4549c4bd3ce0b88d7dbb53",
      "e61cdc75f73e43f3aaa1c5d9cec3acd7",
      "b96c84ab2b72440fbae7f16fc5e685f5",
      "9a8837b735c54778965b561d89681536",
      "5c2affd570ca497188eeda66ff81009e",
      "a077e5853bd64ed18c19b653a8d3d2a1",
      "555fd415bf804ef5a2d07e929d46637a",
      "3417816cd0974a8aab3997af38026024",
      "8b47c9808eba46aa908395a80d17a7ae",
      "9c6854feab25455198d5c875a79b1687",
      "9d753d7f2a0c47c59c8f1ab089dcc492",
      "8d1dc623aa274b35b480964b781bef55",
      "ac17d1d4bfcb4f4a9c56813ba00790ab",
      "8bae3ca3fc2842b9804774810f1b65ac",
      "91307ceea1b04b1e80b128d0ba6a5198",
      "7904483626ae4a70b16562784f1101cc",
      "16b94914d720416987e4088c4712b75f",
      "757eb084656e43a2a9ef1e8a0f732057",
      "ed307e1d8f734648bc164f8018899520",
      "307004224d8b416f95897d9c9bbf7e3d",
      "552b8b2dac414f8c92382f5d8f88eb31",
      "6cd2629ab45d4f6eb66211ecf7f278c4",
      "a47ff535606549fda0ce44f68ddbe716",
      "611172bdd8bc4fed8ce14b447d4b39ea",
      "0b76a3d19b2747c0916ad5ba9977e511",
      "6036bd42f6464ed9b18c46a570a497b3",
      "c3c2b6ee979b44d798cdc59db5a9f92b",
      "3fef9961b9704df38f6a108e942ad6c9",
      "38ed27ded66a4b08977d522c4d74d78c",
      "256e10e2549f41fda46c4cebc94d2982",
      "f624cdd4a93846b0b3dd070f2bc2cc28",
      "e88099cf2d2642188ee6c67db9b5f87f",
      "a119430b092046afa081fbf57bb35523",
      "bdf55349d0604c668bee198b113af5f2",
      "792f118835894f69b91ec4a068fbe80f",
      "1efe595d84d64b04aa973942d4245338",
      "fa62e80e428649f29b3dbef17b02b8a5",
      "efe4755ac9e943acbef2428607f1cd5b",
      "63be17e436724c13844ca07a7ad988ea",
      "ee72db4ade3e430eb7f1c8f55b196385",
      "f26caa06dc34404fbb9477d196b57950",
      "28a169369a35437ba06ed8782f5d3803",
      "c6a63d4bfcde4c2499afaf0a60f3dead",
      "7139585a415a410bb9ebd7787af3d5db",
      "feb7cefbe9b343b680bce77dcc9233ae",
      "b0e76f4901c44353abcb337a1f3ceecb",
      "27e9cc70fc2143daa7e959e8ffac4688",
      "3a0f74165ba541689b3f2fc312ccd281",
      "5cf1e764afdf4819b30d85862decf596",
      "62a3c3a56ef64ac8a290685e48be171a",
      "2bc818c6dfc044a2ae375dcfd60313bb"
     ]
    },
    "id": "YEoaaYAZCDnX",
    "outputId": "1f5810f9-72a2-484b-e243-30f538e3a0ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f609c2a5fd8d4811b9a1b1eea5208ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3417816cd0974a8aab3997af38026024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed307e1d8f734648bc164f8018899520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256e10e2549f41fda46c4cebc94d2982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26caa06dc34404fbb9477d196b57950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coKh9lECcw45"
   },
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "m7IRRMdMiMtN"
   },
   "outputs": [],
   "source": [
    "origin_lang = \"eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjB3SBkqn1JG"
   },
   "source": [
    "#### 1. Con Finetuning Espa√±ol-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uBCVbR3obZG",
    "outputId": "556c2df6-3039-4fdd-f339-bfbb6530c4eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Traduciendo: eng ‚Üí Espa√±ol ‚Üí Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321/321 [10:18<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_intermediate = []\n",
    "references_intermediate = []\n",
    "\n",
    "print(f\"\\nüîπ Traduciendo: {origin_lang} ‚Üí Espa√±ol ‚Üí Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    # Paso 1: origin_lang ‚Üí espa√±ol\n",
    "    inter = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"spa_Latn\")\n",
    "\n",
    "    # Paso 2: espa√±ol ‚Üí \"Sipakapense\"\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, inter, \"spa_Latn\", \"qum\")\n",
    "\n",
    "    predictions_intermediate.append(pred)\n",
    "    references_intermediate.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qVv0v6pUvm-e"
   },
   "outputs": [],
   "source": [
    "clean_pred_intermediate = [pred.removeprefix(\"qum\").strip() for pred in predictions_intermediate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2roPLoJoy2L",
    "outputId": "12939517-55db-47bb-9f4f-1ef4480ba4be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' wife only uses good wood.\n",
      "> Referencia:  wu rxqiil ta º xuux xaq utz laj si º kchuknik chre\n",
      "> Predicci√≥n:  ri ajk ºalob º ri rk ºo ºq\n",
      "\n",
      "> Entrada:     #qum# All the peaches are\n",
      "> Referencia:  Njeel ri xkarawa ºn xpoch ºik rech chenim xchaq ºjik\n",
      "> Predicci√≥n:  q q ºiij ri rq ºiij ri rk ºiij\n",
      "\n",
      "> Entrada:     #qum# Our people don't speak our language anymore.\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolb ºaal\n",
      "> Predicci√≥n:  ri rwi ºjb ºal ri xb ºal\n",
      "\n",
      "> Entrada:     #qum# The student is learning how\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri su º\n",
      "> Predicci√≥n:  rx ºiyik rq ºo ºq ºik\n",
      "\n",
      "> Entrada:     #qum# The woman was hugged by her husband\n",
      "> Referencia:  Ri ixoq tz ºulmaj rum ri rchjiil\n",
      "> Predicci√≥n:  ri rwi ºy ºy ri rb ºan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicci√≥n:  {clean_pred_intermediate[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcM-8kilozFz",
    "outputId": "ffd03a26-2485-4dba-aabc-1e4c5d1d4b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√©tricas BLEU ===\n",
      "BLEU-1: 15.97\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== M√©tricas ROUGE ===\n",
      "ROUGE-1:   15.48\n",
      "ROUGE-2:   0.40\n",
      "ROUGE-L:   15.10\n",
      "ROUGE-Lsum:15.01\n",
      "\n",
      "=== Otras M√©tricas ===\n",
      "METEOR:     8.15\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_intermediate, references=[[ref] for ref in references_intermediate])\n",
    "\n",
    "print(\"\\n=== M√©tricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== M√©tricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Otras M√©tricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MTsr6mJBWrm",
    "outputId": "afa8c30e-8b2d-409c-8866-2cf20af55efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\n"
     ]
    }
   ],
   "source": [
    "with open(f\"results_{lang}_qum_finetune_intermediate.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_intermediate, references_intermediate):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(\"‚úÖ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_UFY-dToFsb"
   },
   "source": [
    "#### 2. Con Finetuning Ingl√©s-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0UrnNGqobuo",
    "outputId": "309c4b76-1a86-43d8-ef33-945825c39444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Traduciendo: eng ‚Üí Sipakapense\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321/321 [04:28<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_direct = []\n",
    "references_direct = []\n",
    "\n",
    "print(f\"\\nüîπ Traduciendo: {origin_lang} ‚Üí Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"qum\")\n",
    "\n",
    "    predictions_direct.append(pred)\n",
    "    references_direct.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "chF42cMCu0k2"
   },
   "outputs": [],
   "source": [
    "clean_pred_direct = [pred.removeprefix(\"qum\").strip() for pred in predictions_direct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGGj_XKctmn9",
    "outputId": "35bbd1e8-7983-4783-c696-a7ec9d40e1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ejemplos Fine-Tuned ===\n",
      "\n",
      "> Entrada:     #qum# Don Jesus' wife only uses good wood.\n",
      "> Referencia:  wu rxqiil ta º xuux xaq utz laj si º kchuknik chre\n",
      "> Predicci√≥n:  ri ajk ºa ºl ri xk ºi ºj\n",
      "\n",
      "> Entrada:     #qum# All the peaches are\n",
      "> Referencia:  Njeel ri xkarawa ºn xpoch ºik rech chenim xchaq ºjik\n",
      "> Predicci√≥n:  ri ajk ºoom\n",
      "\n",
      "> Entrada:     #qum# Our people don't speak our language anymore.\n",
      "> Referencia:  Pri qtinmit qal chik kchkunsxik ri qyolb ºaal\n",
      "> Predicci√≥n:  ri rka ºb ºik ri achii xtz ºaqb ºik\n",
      "\n",
      "> Entrada:     #qum# The student is learning how\n",
      "> Referencia:  Ri ajtijnel tjin kirtijuuj c hemo rchkunsxik ri su º\n",
      "> Predicci√≥n:  ri ajk ºalb ºal wu rka ºyik\n",
      "\n",
      "> Entrada:     #qum# The woman was hugged by her husband\n",
      "> Referencia:  Ri ixoq tz ºulmaj rum ri rchjiil\n",
      "> Predicci√≥n:  ri achii xq ºi ºb ºal ri ajk ºalob º\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicci√≥n:  {clean_pred_direct[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNuiAUOytvUu",
    "outputId": "f81566cd-04ee-4aa0-dde3-3560a6d26c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√©tricas BLEU ===\n",
      "BLEU-1: 16.64\n",
      "BLEU-2: 0.20\n",
      "BLEU-3: 0.00\n",
      "BLEU-4: 0.00\n",
      "BLEU total: 0.00\n",
      "\n",
      "=== M√©tricas ROUGE ===\n",
      "ROUGE-1:   14.83\n",
      "ROUGE-2:   0.59\n",
      "ROUGE-L:   14.21\n",
      "ROUGE-Lsum:14.19\n",
      "\n",
      "=== Otras M√©tricas ===\n",
      "METEOR:     7.44\n"
     ]
    }
   ],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_direct, references=[[ref] for ref in references_direct])\n",
    "\n",
    "print(\"\\n=== M√©tricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== M√©tricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Otras M√©tricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2o0_ZKITBrRf",
    "outputId": "f658e952-a809-4580-b298-3fac6a983ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados guardados en 'results_en_qum_finetune_direct.tsv'\n"
     ]
    }
   ],
   "source": [
    "filename = f\"results_{lang}_qum_finetune_direct.tsv\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_direct, references_direct):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(f\"‚úÖ Resultados guardados en '{filename}'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g5rb6MT0A5mD",
    "R6AMhN2GZpNt",
    "Q5ukIdzqYhCo",
    "o6ngW-l5c1aq",
    "qjWCp3bkc27u",
    "dazLUZlhKKHk",
    "tc2QH1HJcuKn",
    "U9jCIC05CCG_",
    "coKh9lECcw45",
    "ljNvMeB3ezyA",
    "rjB3SBkqn1JG",
    "y_UFY-dToFsb",
    "fCaQTajaoPyr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
